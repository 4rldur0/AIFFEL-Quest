{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0133843",
   "metadata": {},
   "source": [
    "## STEP 1. 형태소 분석기를 이용하여 품사에 따라 해당 단어를 추출하기\n",
    "\n",
    "- 명사만 추출 `nouns.txt`\n",
    "- 명사, 동사 추출 `nouns_verbs.txt`\n",
    "- 명사, 동사, 형용사 추출 `nouns_verbs_adjectives.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70d0dfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71156/71156 [07:09<00:00, 165.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# 시간이 오래 소요되므로 파일로 저장\n",
    "import os\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "okt = Okt()\n",
    "input_file_path = os.getenv('HOME') + '/aiffel/weat/synopsis.txt'\n",
    "output_file_path = './tokenized/nouns_verbs.txt'\n",
    "\n",
    "'''\n",
    "with open(input_file_path, 'r', encoding='utf-8') as rf:\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as wf:\n",
    "        lines = rf.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            words = okt.pos(line, stem=True, norm=True)\n",
    "            res = [w[0] for w in words if w[1] == \"Noun\" or w[1] == \"Verb\"]\n",
    "            wf.write(' '.join(res) + '\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d0bafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=[]\n",
    "with open(output_file_path, 'r', encoding='utf-8') as rf:\n",
    "    for line in rf:\n",
    "        # 각 줄을 읽어 공백을 기준으로 단어를 나누고 리스트로 변환\n",
    "        words = line.strip().split()\n",
    "        tokenized.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d148f0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1723183\n"
     ]
    }
   ],
   "source": [
    "# 개수 세기\n",
    "cnt=0\n",
    "for words in tokenized:\n",
    "    cnt+=len(words)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6eb0b",
   "metadata": {},
   "source": [
    "- Noun 개수: 1342179\n",
    "- Noun & Verb 개수: 1723183\n",
    "- Noun & Verb & Adjective 개수: 1850315"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcb9791",
   "metadata": {},
   "source": [
    "## STEP 2. 추출된 결과로 embedding model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27dd68c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('작품', 0.853316068649292),\n",
       " ('다큐멘터리', 0.8298701643943787),\n",
       " ('드라마', 0.8007744550704956),\n",
       " ('영화로', 0.7821309566497803),\n",
       " ('코미디', 0.7374401688575745),\n",
       " ('가족영화', 0.7299917936325073),\n",
       " ('스토리', 0.7158835530281067),\n",
       " ('감동', 0.7140049338340759),\n",
       " ('영상', 0.7099827527999878),\n",
       " ('주제', 0.7072726488113403)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4886f3b",
   "metadata": {},
   "source": [
    "## STEP 3. target, attribute 단어 셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d557b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF/IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "def save_tokens(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        with open('./tokenized/'+file_name+'_nouns_verbs', 'w', encoding='utf-8') as fwrite:\n",
    "            print(file_name, '파일을 읽고 있습니다.')\n",
    "            lines = fread.readlines()\n",
    "            for line in tqdm(lines):\n",
    "                words = okt.pos(line, stem=True, norm=True)\n",
    "                res = [w[0] for w in words if w[1] == \"Noun\" or w[1] == \"Verb\"]\n",
    "                fwrite.write(' '.join(res) + '\\n')\n",
    "                \n",
    "def read_tokens(file_name):\n",
    "    tokenized=[]\n",
    "    with open('./tokenized/'+file_name+'_nouns_verbs', 'r', encoding='utf-8') as fread:\n",
    "        for line in fread:\n",
    "            # 각 줄을 읽어 공백을 기준으로 단어를 나누고 리스트로 변환\n",
    "            words = line.strip().split()\n",
    "            tokenized.append(words)\n",
    "            \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44150467",
   "metadata": {},
   "source": [
    "### target 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94959801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_art.txt 파일을 읽고 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14435/14435 [01:04<00:00, 224.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_gen.txt 파일을 읽고 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1121/48116 [00:11<06:02, 129.76it/s]"
     ]
    }
   ],
   "source": [
    "# 토큰 파일에 저장\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    " \n",
    "save_tokens(art_txt)\n",
    "save_tokens(gen_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45044f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_art.txt 파일을 읽고 있습니다.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_99/3903954990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgen_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'synopsis_gen.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_txt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_99/2653231382.py\u001b[0m in \u001b[0;36mread_token\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mtokenlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mokt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Noun\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#, \"Adjective\", \"Verb\"]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/konlpy/tag/_okt.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, norm, stem, join)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         tokens = self.jki.tokenize(\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0mphrase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                     \u001b[0mjpype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "art=read_tokens(art_txt)\n",
    "gen=read_tokens(gen_txt)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24781910",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "# target 단어 추출\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "    \n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for word in w1_:\n",
    "    print(word, end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for word in w2_:\n",
    "    print(word, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b48675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "n = 15\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a8fe7",
   "metadata": {},
   "source": [
    "### attribute 단어 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 불러와서 TF-IDF 생성\n",
    "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
    "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
    "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
    "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
    "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
    "         '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지']\n",
    "\n",
    "genre = []\n",
    "for file_name in genre_txt:\n",
    "    genre.append(read_token(file_name))\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6fe34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276bc98a",
   "metadata": {},
   "source": [
    "## STEP 4. WEAT score 계산과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "# WEAT score function\n",
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model과 단어 셋으로 WEAT score 구해보기\n",
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)\n",
    "        \n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0484c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_heatmap(matrix, genre_name, title, save=True):\n",
    "    np.random.seed(0)\n",
    "    # 한글 지원 폰트\n",
    "    sns.set(font='NanumGothic')\n",
    "\n",
    "    # 마이너스 부호 \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,13))\n",
    "    sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True, cmap='RdYlGn_r', ax=ax)\n",
    "\n",
    "    ax.set_title(title, fontsize=20, pad=20)\n",
    "    if save:\n",
    "        plt.savefig('heatmap_'+title+'.png', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_heatmap(matrix, genre_name, 'simple_tfidf', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
