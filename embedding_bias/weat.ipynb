{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a6cab2",
   "metadata": {},
   "source": [
    "## STEP 1. 형태소 분석기를 이용하여 품사가 명사인 경우 해당 단어를 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0749948a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tokenized/nouns.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/3477452219.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tokenized/nouns.txt'"
     ]
    }
   ],
   "source": [
    "# 시간이 오래 소요되므로 파일로 저장\n",
    "import os\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "input_file_path = os.getenv('HOME') + '/aiffel/weat/synopsis.txt'\n",
    "output_file_path = './tokenized/nouns.txt'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as rf:\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as wf:\n",
    "        while True:\n",
    "            line = rf.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            words = okt.pos(line, stem=True, norm=True)\n",
    "            res = [w[0] for w in words if w[1] == \"Noun\"]\n",
    "            wf.write(' '.join(res) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147aa21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, 'r', encoding='utf-8') as rf:\n",
    "    for line in rf:\n",
    "        # 각 줄을 읽어 공백을 기준으로 단어를 나누고 리스트로 변환\n",
    "        words = line.strip().split()\n",
    "        tokenized.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838590d8",
   "metadata": {},
   "source": [
    "## STEP 2. 추출된 결과로 embedding model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d366c2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('작품', 0.886648416519165),\n",
       " ('다큐멘터리', 0.8544517755508423),\n",
       " ('영화로', 0.824195146560669),\n",
       " ('드라마', 0.8082562685012817),\n",
       " ('코미디', 0.8076089024543762),\n",
       " ('시대극', 0.7985746264457703),\n",
       " ('주제', 0.7908185124397278),\n",
       " ('소재', 0.7800697684288025),\n",
       " ('감동', 0.7800178527832031),\n",
       " ('형식', 0.7779371738433838)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af228f",
   "metadata": {},
   "source": [
    "## STEP 3. target, attribute 단어 셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4f5cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_art.txt 파일을 읽고 있습니다.\n",
      "synopsis_gen.txt 파일을 읽고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# TF/IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c21ab08",
   "metadata": {},
   "source": [
    "### target 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fcf6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 불러와서 TF-IDF 생성\n",
    "art = read_token(art_txt)\n",
    "gen = read_token(gen_txt)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18242fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예술영화를 대표하는 단어들:\n",
      "그녀, 자신, 시작, 위해, 사랑, 사람, 영화, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "친구, 남자, 가족, 이야기, 마을, 사건, 마음, 세상, 아버지, 아이, 엄마, 모든, 여자, 대한, 서로, 과연, 다시, 시간, 아들, 소녀, 아내, 다른, 사이, 영화제, 세계, 사실, 하나, 점점, 남편, 감독, 여행, 인생, 발견, 모두, 순간, 우리, 가장, 마지막, 생활, 아빠, 모습, 통해, 죽음, 기억, 비밀, 학교, 음악, 한편, 소년, 생각, 도시, 명의, 사고, 결혼, 전쟁, 때문, 위기, 이제, 최고, 이자, 과거, 일상, 경찰, 상황, 간다, 미국, 결심, 운명, 현실, 관계, 지금, 단편, 여인, 하루, 이름, 이후, 준비, 인간, 감정, 만난, 국제, 처음, 충격, 살인, 누구, 동안, 존재, 그린, 어머니, 연인, 계속, 동생, 작품, \n",
      "\n",
      "일반영화를 대표하는 단어들:\n",
      "자신, 그녀, 영화제, 위해, 사람, 시작, 국제, 영화, 친구, 사랑, 남자, 이야기, 대한, 서울, 여자, 사건, 남편, 아이, 가족, 아버지, 다른, 마을, 시간, 엄마, 아들, 모든, 단편, 마음, 사실, 다시, 세계, 모습, 작품, 통해, 생각, 서로, 세상, 발견, 감독, 아내, 관계, 소녀, 사이, 하나, 우리, 애니메이션, 때문, 여성, 죽음, 과연, 점점, 인간, 생활, 한편, 결혼, 상황, 모두, 기억, 명의, 소년, 여행, 가장, 간다, 순간, 이제, 도시, 비밀, 학교, 과거, 가지, 이자, 경찰, 마지막, 미국, 동안, 전쟁, 주인공, 대해, 존재, 현실, 연출, 사고, 살인, 일상, 어머니, 계속, 사회, 인생, 다큐멘터리, 부문, 섹스, 최고, 바로, 동생, 의도, 하루, 위기, 계획, 정체, 한국, "
     ]
    }
   ],
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "# target 단어 추출\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c73d3",
   "metadata": {},
   "source": [
    "### attribute 단어 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 불러와서 TF-IDF 생성\n",
    "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
    "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
    "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
    "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
    "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
    "         '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지']\n",
    "genre = []\n",
    "for file_name in genre_txt:\n",
    "    genre.append(read_token(file_name))\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60953f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synopsis_SF.txt 파일을 읽고 있습니다.\n",
      "synopsis_family.txt 파일을 읽고 있습니다.\n",
      "synopsis_show.txt 파일을 읽고 있습니다.\n",
      "synopsis_horror.txt 파일을 읽고 있습니다.\n",
      "synopsis_etc.txt 파일을 읽고 있습니다.\n",
      "synopsis_documentary.txt 파일을 읽고 있습니다.\n",
      "synopsis_drama.txt 파일을 읽고 있습니다.\n",
      "synopsis_romance.txt 파일을 읽고 있습니다.\n",
      "synopsis_musical.txt 파일을 읽고 있습니다.\n",
      "synopsis_mystery.txt 파일을 읽고 있습니다.\n",
      "synopsis_crime.txt 파일을 읽고 있습니다.\n",
      "synopsis_historical.txt 파일을 읽고 있습니다.\n",
      "synopsis_western.txt 파일을 읽고 있습니다.\n",
      "synopsis_adult.txt 파일을 읽고 있습니다.\n",
      "synopsis_thriller.txt 파일을 읽고 있습니다.\n",
      "synopsis_animation.txt 파일을 읽고 있습니다.\n",
      "synopsis_action.txt 파일을 읽고 있습니다.\n",
      "synopsis_adventure.txt 파일을 읽고 있습니다.\n",
      "synopsis_war.txt 파일을 읽고 있습니다.\n",
      "synopsis_comedy.txt 파일을 읽고 있습니다.\n",
      "synopsis_fantasy.txt 파일을 읽고 있습니다.\n",
      "SF: 위해, 자신, 지구, 시작, 사람, 인류, 인간, 미래, 우주, 그녀, 로봇, 세계, 모든, 박사, 우주선, \n",
      "가족: 엄마, 아빠, 가족, 영화제, 자신, 위해, 친구, 아주르, 아버지, 시작, 그녀, 아들, 마을, 국제, 낙타, \n",
      "공연: 오페라, 사랑, 토스카, 실황, 올레, 자신, 카바, 그녀, 공연, 오텔로, 리골레토, 백작, 프레, 베르디, 위해, \n",
      "공포(호러): 시작, 위해, 사람, 자신, 친구, 그녀, 사건, 공포, 발견, 죽음, 마을, 가족, 악령, 남자, 좀비, \n",
      "기타: 영화제, 국제, 서울, 단편, 영화, 자신, 사람, 이야기, 그녀, 남자, 위해, 시작, 사랑, 뉴미디어, 페스티벌, \n",
      "다큐멘터리: 영화제, 영화, 다큐, 국제, 다큐멘터리, 사람, 이야기, 대한, 자신, 감독, 위해, 서울, 우리, 시작, 세계, \n",
      "드라마: 자신, 영화제, 그녀, 사람, 사랑, 영화, 위해, 시작, 국제, 남자, 친구, 이야기, 엄마, 여자, 아버지, \n",
      "멜로로맨스: 그녀, 사랑, 자신, 시작, 남편, 남자, 여자, 사람, 친구, 섹스, 위해, 마음, 결혼, 서로, 아내, \n",
      "뮤지컬: 뮤지컬, 사랑, 에스메랄다, 그녀, 음악, 충무로, 모차르트, 영화, 토스카, 자신, 니웨, 카바, 영화제, 바흐, 페뷔스, \n",
      "미스터리: 사건, 그녀, 시작, 자신, 위해, 사람, 발견, 사고, 진실, 죽음, 기억, 살인, 친구, 아내, 남자, \n",
      "범죄: 사건, 위해, 자신, 경찰, 시작, 그녀, 범죄, 조직, 살인, 사람, 마약, 형사, 남자, 모든, 살해, \n",
      "사극: 조선, 위해, 시작, 신기전, 사랑, 자신, 아가멤논, 황제, 그녀, 루안, 최고, 운명, 사람, 하선, 전쟁, \n",
      "서부극(웨스턴): 서부, 보안관, 위해, 벌린, 카우보이, 그레이프바인, 헨리, 마을, 자신, 개릿, 아이, 시작, 무법자, 프린트, 마적, \n",
      "성인물(에로): 그녀, 남편, 마사지, 자신, 섹스, 관계, 영화, 정사, 남자, 위해, 시작, 여자, 유부녀, 마음, 사랑, \n",
      "스릴러: 자신, 그녀, 사건, 시작, 위해, 사람, 살인, 남자, 발견, 아내, 경찰, 친구, 모든, 사실, 살해, \n",
      "애니메이션: 애니메이션, 국제, 영화제, 친구, 인디애니페스트, 위해, 자신, 시작, 사람, 페스티벌, 서울, 이야기, 아이, 마을, 소녀, \n",
      "액션: 위해, 자신, 시작, 조직, 사건, 사람, 그녀, 경찰, 전쟁, 모든, 목숨, 사실, 친구, 가족, 요원, \n",
      "어드벤처: 위해, 자신, 시작, 친구, 마을, 아버지, 영화, 아이, 사람, 여행, 세계, 앤트, 세상, 가족, 모험, \n",
      "전쟁: 전쟁, 독일군, 전투, 위해, 작전, 시작, 부대, 윈터스, 독일, 연합군, 미군, 임무, 자신, 사람, 나치, \n",
      "코미디: 그녀, 자신, 시작, 위해, 사랑, 사람, 친구, 영화, 남자, 여자, 영화제, 가족, 과연, 마을, 사건, \n",
      "판타지: 자신, 그녀, 시작, 위해, 사람, 사랑, 요괴, 영화제, 이야기, 영화, 소녀, 남자, 인간, 세상, 마을, \n"
     ]
    }
   ],
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6108ea",
   "metadata": {},
   "source": [
    "## STEP 4. WEAT score 계산과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03814ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "# WEAT score function\n",
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d367328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SF 가족 -0.5250815\n",
      "SF 공연 -0.38076147\n",
      "SF 공포(호러) -0.6964674\n",
      "SF 기타 0.35148215\n",
      "SF 다큐멘터리 0.5931078\n",
      "SF 드라마 -0.38868818\n",
      "SF 멜로로맨스 -0.8079767\n",
      "SF 뮤지컬 0.32724223\n",
      "SF 미스터리 -0.7034123\n",
      "SF 범죄 -0.26127744\n",
      "SF 사극 -0.836256\n",
      "SF 서부극(웨스턴) -0.42475638\n",
      "SF 성인물(에로) -0.58250743\n",
      "SF 스릴러 -0.5395713\n",
      "SF 애니메이션 0.36044207\n",
      "SF 액션 -0.4819872\n",
      "SF 어드벤처 -0.8033882\n",
      "SF 전쟁 0.22449192\n",
      "SF 코미디 -0.45639727\n",
      "SF 판타지 -0.20104498\n",
      "가족 공연 0.20113707\n",
      "가족 공포(호러) -0.17772157\n",
      "가족 기타 0.75772876\n",
      "가족 다큐멘터리 0.8397989\n",
      "가족 드라마 0.22676712\n",
      "가족 멜로로맨스 -0.63209766\n",
      "가족 뮤지컬 0.7046117\n",
      "가족 미스터리 -0.32887682\n",
      "가족 범죄 0.18669163\n",
      "가족 사극 -0.08369598\n",
      "가족 서부극(웨스턴) 0.445031\n",
      "가족 성인물(에로) -0.32356948\n",
      "가족 스릴러 -0.15422046\n",
      "가족 애니메이션 0.8597249\n",
      "가족 액션 0.08878305\n",
      "가족 어드벤처 -0.022493953\n",
      "가족 전쟁 0.66804725\n",
      "가족 코미디 0.1990872\n",
      "가족 판타지 0.4279034\n",
      "공연 공포(호러) -0.26817703\n",
      "공연 기타 0.934007\n",
      "공연 다큐멘터리 0.9179254\n",
      "공연 드라마 0.0092852535\n",
      "공연 멜로로맨스 -0.73806673\n",
      "공연 뮤지컬 0.87697643\n",
      "공연 미스터리 -0.3829619\n",
      "공연 범죄 0.05396664\n",
      "공연 사극 -0.39449522\n",
      "공연 서부극(웨스턴) 0.049652636\n",
      "공연 성인물(에로) -0.5053268\n",
      "공연 스릴러 -0.239368\n",
      "공연 애니메이션 0.9082278\n",
      "공연 액션 -0.05486571\n",
      "공연 어드벤처 -0.21721497\n",
      "공연 전쟁 0.61364627\n",
      "공연 코미디 -0.011284757\n",
      "공연 판타지 0.32304382\n",
      "공포(호러) 기타 0.6604173\n",
      "공포(호러) 다큐멘터리 0.76050055\n",
      "공포(호러) 드라마 0.33328378\n",
      "공포(호러) 멜로로맨스 -0.62742287\n",
      "공포(호러) 뮤지컬 0.62662345\n",
      "공포(호러) 미스터리 -0.58844334\n",
      "공포(호러) 범죄 0.62148446\n",
      "공포(호러) 사극 0.13077457\n",
      "공포(호러) 서부극(웨스턴) 0.4388056\n",
      "공포(호러) 성인물(에로) -0.21571955\n",
      "공포(호러) 스릴러 -0.018094622\n",
      "공포(호러) 애니메이션 0.6756171\n",
      "공포(호러) 액션 0.5304039\n",
      "공포(호러) 어드벤처 0.20171846\n",
      "공포(호러) 전쟁 0.6755987\n",
      "공포(호러) 코미디 0.45311964\n",
      "공포(호러) 판타지 0.54409385\n",
      "기타 다큐멘터리 0.5832642\n",
      "기타 드라마 -0.9050784\n",
      "기타 멜로로맨스 -0.9126143\n",
      "기타 뮤지컬 -0.1284691\n",
      "기타 미스터리 -0.6813247\n",
      "기타 범죄 -0.40691134\n",
      "기타 사극 -0.85302114\n",
      "기타 서부극(웨스턴) -0.6312541\n",
      "기타 성인물(에로) -0.8397214\n",
      "기타 스릴러 -0.5824182\n",
      "기타 애니메이션 -0.08570762\n",
      "기타 액션 -0.5022481\n",
      "기타 어드벤처 -0.8147755\n",
      "기타 전쟁 -0.22687891\n",
      "기타 코미디 -0.7560001\n",
      "기타 판타지 -0.71045\n",
      "다큐멘터리 드라마 -0.9335758\n",
      "다큐멘터리 멜로로맨스 -0.9143046\n",
      "다큐멘터리 뮤지컬 -0.7471028\n",
      "다큐멘터리 미스터리 -0.7640064\n",
      "다큐멘터리 범죄 -0.520311\n",
      "다큐멘터리 사극 -0.91939694\n",
      "다큐멘터리 서부극(웨스턴) -0.736623\n",
      "다큐멘터리 성인물(에로) -0.84923196\n",
      "다큐멘터리 스릴러 -0.6663515\n",
      "다큐멘터리 애니메이션 -0.55585915\n",
      "다큐멘터리 액션 -0.62248975\n",
      "다큐멘터리 어드벤처 -0.95508635\n",
      "다큐멘터리 전쟁 -0.4346502\n",
      "다큐멘터리 코미디 -0.8390345\n",
      "다큐멘터리 판타지 -0.90602577\n",
      "드라마 멜로로맨스 -0.8543596\n",
      "드라마 뮤지컬 0.83931303\n",
      "드라마 미스터리 -0.44928765\n",
      "드라마 범죄 0.05267622\n",
      "드라마 사극 -0.36955968\n",
      "드라마 서부극(웨스턴) 0.04143044\n",
      "드라마 성인물(에로) -0.60560864\n",
      "드라마 스릴러 -0.27161297\n",
      "드라마 애니메이션 0.892435\n",
      "드라마 액션 -0.062002327\n",
      "드라마 어드벤처 -0.2998097\n",
      "드라마 전쟁 0.48663473\n",
      "드라마 코미디 -0.036386605\n",
      "드라마 판타지 0.5413151\n",
      "멜로로맨스 뮤지컬 0.8786054\n",
      "멜로로맨스 미스터리 0.44347402\n",
      "멜로로맨스 범죄 0.8451779\n",
      "멜로로맨스 사극 0.6703473\n",
      "멜로로맨스 서부극(웨스턴) 0.7642132\n",
      "멜로로맨스 성인물(에로) 0.841565\n",
      "멜로로맨스 스릴러 0.6544604\n",
      "멜로로맨스 애니메이션 0.88960326\n",
      "멜로로맨스 액션 0.74835\n",
      "멜로로맨스 어드벤처 0.61822414\n",
      "멜로로맨스 전쟁 0.8247105\n",
      "멜로로맨스 코미디 0.920377\n",
      "멜로로맨스 판타지 0.8494049\n",
      "뮤지컬 미스터리 -0.65353125\n",
      "뮤지컬 범죄 -0.37773556\n",
      "뮤지컬 사극 -0.81238055\n",
      "뮤지컬 서부극(웨스턴) -0.5774406\n",
      "뮤지컬 성인물(에로) -0.80459446\n",
      "뮤지컬 스릴러 -0.55268484\n",
      "뮤지컬 애니메이션 -7.98964e-05\n",
      "뮤지컬 액션 -0.47419825\n",
      "뮤지컬 어드벤처 -0.7815582\n",
      "뮤지컬 전쟁 -0.19101581\n",
      "뮤지컬 코미디 -0.7001868\n",
      "뮤지컬 판타지 -0.66721714\n",
      "미스터리 범죄 1.0180056\n",
      "미스터리 사극 0.30977684\n",
      "미스터리 서부극(웨스턴) 0.5361306\n",
      "미스터리 성인물(에로) -0.023204748\n",
      "미스터리 스릴러 0.80548316\n",
      "미스터리 애니메이션 0.6858818\n",
      "미스터리 액션 0.79941237\n",
      "미스터리 어드벤처 0.34795648\n",
      "미스터리 전쟁 0.70751077\n",
      "미스터리 코미디 0.56054246\n",
      "미스터리 판타지 0.5887928\n",
      "범죄 사극 -0.28560245\n",
      "범죄 서부극(웨스턴) -0.039737295\n",
      "범죄 성인물(에로) -0.4821412\n",
      "범죄 스릴러 -0.95997226\n",
      "범죄 애니메이션 0.40165403\n",
      "범죄 액션 -0.32976502\n",
      "범죄 어드벤처 -0.1996378\n",
      "범죄 전쟁 0.411319\n",
      "범죄 코미디 -0.07820803\n",
      "범죄 판타지 0.119190075\n",
      "사극 서부극(웨스턴) 0.43881854\n",
      "사극 성인물(에로) -0.32861912\n",
      "사극 스릴러 -0.10973551\n",
      "사극 애니메이션 0.85204744\n",
      "사극 액션 0.1887688\n",
      "사극 어드벤처 0.097098194\n",
      "사극 전쟁 0.9074501\n",
      "사극 코미디 0.450508\n",
      "사극 판타지 0.6626603\n",
      "서부극(웨스턴) 성인물(에로) -0.45895368\n",
      "서부극(웨스턴) 스릴러 -0.35855654\n",
      "서부극(웨스턴) 애니메이션 0.69523686\n",
      "서부극(웨스턴) 액션 -0.13176186\n",
      "서부극(웨스턴) 어드벤처 -0.38503125\n",
      "서부극(웨스턴) 전쟁 0.68147457\n",
      "서부극(웨스턴) 코미디 -0.06767553\n",
      "서부극(웨스턴) 판타지 0.24225684\n",
      "성인물(에로) 스릴러 0.20763502\n",
      "성인물(에로) 애니메이션 0.7827409\n",
      "성인물(에로) 액션 0.37771\n",
      "성인물(에로) 어드벤처 0.31676713\n",
      "성인물(에로) 전쟁 0.6319054\n",
      "성인물(에로) 코미디 0.60855615\n",
      "성인물(에로) 판타지 0.6501396\n",
      "스릴러 애니메이션 0.58532083\n",
      "스릴러 액션 0.5182983\n",
      "스릴러 어드벤처 0.14688013\n",
      "스릴러 전쟁 0.60896224\n",
      "스릴러 코미디 0.33304718\n",
      "스릴러 판타지 0.413448\n",
      "애니메이션 액션 -0.5120125\n",
      "애니메이션 어드벤처 -0.8958107\n",
      "애니메이션 전쟁 -0.22072724\n",
      "애니메이션 코미디 -0.75009495\n",
      "애니메이션 판타지 -0.7020041\n",
      "액션 어드벤처 -0.110089056\n",
      "액션 전쟁 0.5868393\n",
      "액션 코미디 0.06663699\n",
      "액션 판타지 0.25177947\n",
      "어드벤처 전쟁 0.7073814\n",
      "어드벤처 코미디 0.32278123\n",
      "어드벤처 판타지 0.7406012\n",
      "전쟁 코미디 -0.53208774\n",
      "전쟁 판타지 -0.29795432\n",
      "코미디 판타지 0.52915126\n"
     ]
    }
   ],
   "source": [
    "# embedding model과 단어 셋으로 WEAT score 구해보기\n",
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)\n",
    "        \n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68052e32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/117434346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mplot_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'simple_tfidf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# 시각화\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_heatmap(matrix, genre_name, title, save=True):\n",
    "    np.random.seed(0)\n",
    "    # 한글 지원 폰트\n",
    "    sns.set(font='NanumGothic')\n",
    "\n",
    "    # 마이너스 부호 \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,13))\n",
    "    sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True, cmap='RdYlGn_r', ax=ax)\n",
    "\n",
    "    ax.set_title(title, fontsize=20, pad=20)\n",
    "    if save:\n",
    "        plt.savefig('heatmap_'+title+'.png', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_heatmap(matrix, genre_name, 'simple_tfidf', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
