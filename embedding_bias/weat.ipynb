{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b86c32",
   "metadata": {},
   "source": [
    "## STEP 1. 형태소 분석기를 이용하여 품사에 따라 해당 단어를 추출하기\n",
    "\n",
    "- 명사만 추출 `nouns.txt`\n",
    "- 명사, 동사 추출 `nouns_verbs.txt`\n",
    "- 명사, 동사, 형용사 추출 `nouns_verbs_adjectives.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2866b887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(input_file_path, \\'r\\', encoding=\\'utf-8\\') as rf:\\n    with open(output_file_path, \\'w\\', encoding=\\'utf-8\\') as wf:\\n        lines = rf.readlines()\\n        for line in tqdm(lines):\\n            words = okt.pos(line, stem=True, norm=True)\\n            res = [w[0] for w in words if w[1] == \"Noun\" or w[1] == \"Verb\"]\\n            wf.write(\\' \\'.join(res) + \\'\\n\\')\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시간이 오래 소요되므로 파일로 저장\n",
    "# word2vec 생성을 위한 토큰화\n",
    "import os\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "okt = Okt()\n",
    "input_file_path = os.getenv('HOME') + '/aiffel/weat/synopsis.txt'\n",
    "output_file_path = './tokenized/nouns.txt'\n",
    "\n",
    "'''\n",
    "with open(input_file_path, 'r', encoding='utf-8') as rf:\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as wf:\n",
    "        lines = rf.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            words = okt.pos(line, stem=True, norm=True)\n",
    "            res = [w[0] for w in words if w[1] == \"Noun\" or w[1] == \"Verb\"]\n",
    "            wf.write(' '.join(res) + '\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c969a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장해놓은 토큰 파일 불러오기\n",
    "tokenized=[]\n",
    "with open(output_file_path, 'r', encoding='utf-8') as rf:\n",
    "    for line in rf:\n",
    "        # 각 줄을 읽어 공백을 기준으로 단어를 나누고 리스트로 변환\n",
    "        words = line.strip().split()\n",
    "        tokenized.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d2b07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1342179\n"
     ]
    }
   ],
   "source": [
    "# 개수 세기\n",
    "cnt=0\n",
    "for words in tokenized:\n",
    "    cnt+=len(words)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e6966",
   "metadata": {},
   "source": [
    "- Noun 개수: 1342179\n",
    "- Noun & Verb 개수: 1723183\n",
    "- Noun & Verb & Adjective 개수: 1850315"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834731d",
   "metadata": {},
   "source": [
    "## STEP 2. 추출된 결과로 embedding model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ec2af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('작품', 0.8872395753860474),\n",
       " ('다큐멘터리', 0.8389464020729065),\n",
       " ('드라마', 0.812419056892395),\n",
       " ('코미디', 0.7906116843223572),\n",
       " ('영화로', 0.7843707799911499),\n",
       " ('형식', 0.780301034450531),\n",
       " ('주제', 0.7609272599220276),\n",
       " ('실화', 0.760643720626831),\n",
       " ('소재', 0.7489678859710693),\n",
       " ('스토리', 0.748929500579834)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e858c",
   "metadata": {},
   "source": [
    "## STEP 3. target, attribute 단어 셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b492ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 토큰화 파일 저장 함수\n",
    "def save_tokens(input_file_name, output_file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+input_file_name, 'r') as fread: \n",
    "        with open('./tokenized/'+output_file_name, 'w', encoding='utf-8') as fwrite:\n",
    "            print(input_file_name, '파일을 읽고 있습니다.')\n",
    "            lines = fread.readlines()\n",
    "            for line in tqdm(lines):\n",
    "                words = okt.pos(line, stem=True, norm=True)\n",
    "                res = [w[0] for w in words if w[1] == \"Noun\" or w[1] == \"Verb\"]\n",
    "                fwrite.write(' '.join(res) + '\\n')\n",
    "# 토큰 파일 불러오는 함수              \n",
    "def read_tokens(file_name):\n",
    "    with open('./tokenized/' + file_name, 'r', encoding='utf-8') as fread:\n",
    "        # 파일 전체를 읽어서 줄바꿈 문자를 제거하고 하나의 문자열로 반환\n",
    "        tokenized = fread.read().replace('\\n', ' ')\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452ff1d",
   "metadata": {},
   "source": [
    "명사만 추출할 때에는 `okt.nouns()`을 활용해보았지만 소요 시간 차이 크지 않았음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e6365d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['하다', '되다', '그녀', '자신', '위해', '않다', '이다', '되어다', '싶다', '점점', '때문', '오다', '가다', '과연', '이제', '통해', '시작', '영화']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 불러오기\n",
    "with open('./stopwords.txt', 'r', encoding='utf-8') as fread:\n",
    "    stopwords = [line.strip() for line in fread.readlines()]\n",
    "    print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f382a073",
   "metadata": {},
   "source": [
    "stopwords 구성 기준: 모든 데이터셋에서 자주 등장하고, 주제와 관련 없는 단어 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1095d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA 결과 출력 함수\n",
    "# 각 주제에 대한 단어 분포 출력\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic #{topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "# 각 문서에서 가장 확률이 큰 주제와 그 확률 출력\n",
    "def print_max_topic_for_documents(model, documents):\n",
    "    topic_distribution = model.transform(X)\n",
    "    max_topic_idxs=[]\n",
    "    for i, doc in enumerate(documents):\n",
    "        max_topic_idx = np.argmax(topic_distribution[i])\n",
    "        max_prob = topic_distribution[i, max_topic_idx]\n",
    "        print(f\"Document #{i}:\")\n",
    "        print(f\"Max Probability Topic: {max_topic_idx}, Probability: {max_prob:.4f}\")\n",
    "        print()\n",
    "        max_topic_idxs.append(max_topic_idx)\n",
    "    return max_topic_idxs\n",
    "        \n",
    "        \n",
    "def get_topic_words(model, topic_idx, feature_names, n_top_words):\n",
    "    topic_word_distribution = model.components_[topic_idx]\n",
    "    top_word_indices = topic_word_distribution.argsort()[:-n_top_words-1:-1]\n",
    "    top_words = [feature_names[i] for i in top_word_indices]\n",
    "    top_word_probs = [topic_word_distribution[i] for i in top_word_indices]\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34039dcd",
   "metadata": {},
   "source": [
    "### target 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871a89d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 데이터 불러오기\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "# 품사에 따라 바꾸기\n",
    "output_art_txt = 'synopsis_art_nouns.txt'\n",
    "output_gen_txt = 'synopsis_gen_nouns.txt'\n",
    "\n",
    "'''\n",
    "save_tokens(art_txt, output_art_txt)\n",
    "save_tokens(gen_txt, output_gen_txt)\n",
    "'''\n",
    "\n",
    "art=read_tokens(output_art_txt)\n",
    "gen=read_tokens(output_gen_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14594e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화, 불용어 제거\n",
    "vec_type = 'tfidf'\n",
    "\n",
    "# TF-IDF\n",
    "if vec_type=='tfidf':\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "# DTM\n",
    "elif vec_type=='dtm':\n",
    "    vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "    \n",
    "X = vectorizer.fit_transform([art,gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92350981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예술영화를 대표하는 단어들:\n",
      "사랑, 사람, 친구, 남자, 가족, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이야기, 마을, 사건, 마음, 세상, 아버지, 아이, 엄마, 모든, 여자, 대한, 서로, 다시, 시간, 아들, 소녀, 아내, 다른, 사이, 영화제, 세계, 사실, 하나, 남편, 감독, 여행, 인생, 발견, 모두, 순간, 우리, 가장, 마지막, 생활, 아빠, 모습, 죽음, 기억, 비밀, 학교, 음악, 한편, 소년, 생각, 도시, 명의, 사고, 결혼, 전쟁, 위기, 최고, 이자, 과거, 일상, 경찰, 상황, 간다, 미국, 결심, 운명, 현실, 관계, 지금, 단편, 여인, 하루, 이름, 이후, 준비, 인간, 감정, 만난, 국제, 처음, 충격, 살인, 누구, 동안, 존재, 그린, 어머니, 연인, 계속, 동생, 작품, 청년, 한국, 가지, 상처, 할머니, 목숨, 이상, 희망, 계획, 매력, \n",
      "\n",
      "일반영화를 대표하는 단어들:\n",
      "영화제, 사람, 국제, 친구, 사랑, 남자, 이야기, 대한, 서울, 여자, 사건, 남편, 아이, 가족, 아버지, 다른, 마을, 시간, 엄마, 아들, 모든, 단편, 마음, 사실, 다시, 세계, 모습, 작품, 생각, 서로, 세상, 발견, 감독, 아내, 관계, 소녀, 사이, 하나, 우리, 애니메이션, 여성, 죽음, 인간, 생활, 한편, 결혼, 상황, 모두, 기억, 명의, 소년, 여행, 가장, 간다, 순간, 도시, 비밀, 학교, 과거, 가지, 이자, 경찰, 마지막, 미국, 동안, 전쟁, 주인공, 대해, 존재, 현실, 연출, 사고, 살인, 일상, 어머니, 계속, 사회, 인생, 다큐멘터리, 부문, 섹스, 최고, 바로, 동생, 의도, 하루, 위기, 계획, 정체, 한국, 이후, 조직, 회사, 보고, 부산, 아빠, 부부, 일본, 문제, 처음, "
     ]
    }
   ],
   "source": [
    "m1 = X[0].tocoo()   # sparse matrix를 가져오기\n",
    "m2 = X[1].tocoo()   # sparse matrix를 가져오기\n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 높은 순으로 정렬\n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 높은 순으로 정렬 \n",
    "\n",
    "w1_, w2_ = [], []\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    word = vectorizer.get_feature_names()[w1[i][0]]\n",
    "    print(word, end=', ')\n",
    "    w1_.append(word)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    word = vectorizer.get_feature_names()[w2[i][0]]\n",
    "    print(word, end=', ')\n",
    "    w2_.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb706cd0",
   "metadata": {},
   "source": [
    "> Q. 왜 두 문서 사이에 중복값이 많을까?(하다, 되다, 그녀, 자신, 위해 등)  \n",
    "> A. 둘 다 영화를 다루고 있으므로 공통된 어휘가 많이 사용됨. 또한 현재 문서가 2개만 존재하므로 단어 간의 IDF 값 차이가 적어져 TF 값이 TF-IDF 값에 영향을 끼치는 정도가 커짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba236cfc",
   "metadata": {},
   "source": [
    "#### TF-IDF/DTM 그대로 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7025a23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "n = 15\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fac9331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예술영화를 대표하는 단어들:\n",
      "음악, 결심, 운명, 지금, 여인, 이름, 준비, 감정, 만난, 충격, 누구, 그린, 연인, 청년, 상처, \n",
      "\n",
      "일반영화를 대표하는 단어들:\n",
      "서울, 애니메이션, 여성, 주인공, 대해, 연출, 사회, 다큐멘터리, 부문, 섹스, 바로, 의도, 정체, 조직, 회사, "
     ]
    }
   ],
   "source": [
    "print('예술영화를 대표하는 단어들:')\n",
    "for word in target_art:\n",
    "    print(word, end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for word in target_gen:\n",
    "    print(word, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad554cd",
   "metadata": {},
   "source": [
    "#### LDA 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abf703ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 41088)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3427826c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00558322, 0.00558358, 0.00558402, 0.97766499, 0.00558419],\n",
       "       [0.00556495, 0.00556564, 0.00556601, 0.97773719, 0.00556621]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=2, \n",
    "                                      learning_method='online', \n",
    "                                      random_state=777, \n",
    "                                      max_iter=10)\n",
    "\n",
    "lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a659d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "클린콘텐츠 강혁 한펑 키프로스 에스포 에리얼 혹세 토사 정작 갈수록 메스 들락거렸다 유동 대리시 리카르도 박사 테리블 별미 조작 깅코 아널드 마크 각성 신동 태세 쿠오 종강 세익스피어 러브스토리 라이트\n",
      "Topic #1:\n",
      "충언 불충분 판이 카위 복장 호전 메이든 지건뎃 주스 리셋 골수 클리닝 제주시 실질 흥수 조준 커즌스 어인 코브햄 훗카이도 무마 노동운동가 후지야 대학교수 완만 고고학자 저능 벼랑 캠벨 로소\n",
      "Topic #2:\n",
      "베라크루즈 김현주 정경 결승전 취소 영준 은행 국가안보국 세실리아 학도 무르만스크 스트라이커 후퇴 나카마 대머리 이노 점정 스페어 펠레우스 홀딩 살갑 서고 까뮈 최민석 승과 모우 아웃팅 훔볼트 보루 철저\n",
      "Topic #3:\n",
      "사람 사랑 영화제 친구 남자 이야기 국제 가족 대한 아이 여자 다른 아버지 세상 마을 소녀 남편 마음 단편 모든 시간 사실 결혼 사건 간다 다시 아내 순간 감독 서로\n",
      "Topic #4:\n",
      "단도 퀴퍼스 주물 종군위안부 허일 모진 미소노 용맹 고요 인승 마리아 브리즈번 저수지 바오산 레오파드 델우드 사냥터 에스콘디도 유혹 관태 고민 맥퍼슨 빈자 음경 주영철 부처 너구리 미남자 라자스탄 고려\n",
      "\n",
      "Document-Topic Distributions:\n",
      "Document #0:\n",
      "Max Probability Topic: 3, Probability: 0.9777\n",
      "\n",
      "Document #1:\n",
      "Max Probability Topic: 3, Probability: 0.9777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LDA의 결과 토픽과 각 단어의 비중을 출력\n",
    "n_top_words = 30\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Topics in LDA model:\")\n",
    "print_top_words(lda_model, tf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Document-Topic Distributions:\")\n",
    "documents = [art, gen]\n",
    "max_topic_idxs = print_max_topic_for_documents(lda_model, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_art = get_topic_words(lda_model, max_topic_idxs[0], tf_feature_names, 15)\n",
    "target_gen = get_topic_words(lda_model, max_topic_idxs[1], tf_feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc66e569",
   "metadata": {},
   "source": [
    "### attribute 단어 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76181996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 불러와서 TF-IDF 생성\n",
    "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
    "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
    "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
    "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
    "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
    "         '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지']\n",
    "# 품사에 따라 바꾸기\n",
    "output_genre_txt = [txt.strip('.txt')+'_nouns.txt' for txt in genre_txt]\n",
    "\n",
    "genre = []\n",
    "for file_name, output_file_name in zip(genre_txt, output_genre_txt):\n",
    "    #save_tokens(file_name, output_file_name)\n",
    "    genre.append(read_tokens(output_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de0b5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터화, 불용어 제거\n",
    "# TF-IDF\n",
    "if vec_type=='tfidf':\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "# DTM\n",
    "elif vec_type=='dtm':\n",
    "    vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(genre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b7b88c",
   "metadata": {},
   "source": [
    "#### TF-IDF/DTM 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b45e2386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SF: 지구, 사람, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인류, 인간, 미래, 우주, 로봇, 세계, 모든, 박사, 우주선, 외계, 존재, 세상, 발견, \n",
      "가족: 엄마, 아빠, 가족, 영화제, 친구, 아주르, 아버지, 아들, 마을, 국제, 낙타, 할머니, 씨제이, 동구, 사랑, \n",
      "공연: 오페라, 사랑, 토스카, 실황, 올레, 카바, 공연, 오텔로, 리골레토, 백작, 프레, 베르디, 카르피, 비바, 왕자, \n",
      "공포(호러): 사람, 친구, 사건, 공포, 발견, 죽음, 마을, 가족, 악령, 남자, 좀비, 사실, 소녀, 하나, 살인, \n",
      "기타: 영화제, 국제, 서울, 단편, 사람, 이야기, 남자, 사랑, 뉴미디어, 페스티벌, 여자, 대한, 독립, 친구, 작품, \n",
      "다큐멘터리: 영화제, 다큐, 국제, 다큐멘터리, 사람, 이야기, 대한, 감독, 서울, 우리, 세계, 여성, 가족, 한국, 작품, \n",
      "드라마: 영화제, 사람, 사랑, 국제, 남자, 친구, 이야기, 엄마, 여자, 아버지, 가족, 단편, 서울, 대한, 아들, \n",
      "멜로로맨스: 사랑, 남편, 남자, 여자, 사람, 친구, 섹스, 마음, 결혼, 서로, 아내, 관계, 부부, 엄마, 아버지, \n",
      "뮤지컬: 뮤지컬, 사랑, 에스메랄다, 음악, 충무로, 모차르트, 토스카, 니웨, 카바, 영화제, 바흐, 페뷔스, 프롤, 모도, 카르피, \n",
      "미스터리: 사건, 사람, 발견, 사고, 진실, 죽음, 기억, 살인, 친구, 아내, 남자, 아이, 민혁, 사실, 의문, \n",
      "범죄: 사건, 경찰, 범죄, 조직, 살인, 사람, 마약, 형사, 남자, 모든, 살해, 수사, 발견, 한길수, 범인, \n",
      "사극: 조선, 신기전, 사랑, 아가멤논, 황제, 루안, 최고, 운명, 사람, 하선, 전쟁, 윤서, 트로이, 세자, 허균, \n",
      "서부극(웨스턴): 서부, 보안관, 벌린, 카우보이, 그레이프바인, 헨리, 마을, 개릿, 아이, 무법자, 프린트, 마적, 태구, 현상금, 분노, \n",
      "성인물(에로): 남편, 마사지, 섹스, 관계, 정사, 남자, 여자, 유부녀, 마음, 사랑, 에피소드, 그린, 아내, 다시, 자위, \n",
      "스릴러: 사건, 사람, 살인, 남자, 발견, 아내, 경찰, 친구, 모든, 사실, 살해, 가족, 형사, 비밀, 사랑, \n",
      "애니메이션: 애니메이션, 국제, 영화제, 친구, 인디애니페스트, 사람, 페스티벌, 서울, 이야기, 아이, 마을, 소녀, 세계, 세상, 애니, \n",
      "액션: 조직, 사건, 사람, 경찰, 전쟁, 모든, 목숨, 사실, 친구, 가족, 요원, 임무, 범죄, 세계, 살인, \n",
      "어드벤처: 친구, 마을, 아버지, 아이, 사람, 여행, 세계, 앤트, 세상, 가족, 모험, 비밀, 대한, 이야기, 모든, \n",
      "전쟁: 전쟁, 독일군, 전투, 작전, 부대, 윈터스, 독일, 연합군, 미군, 임무, 사람, 나치, 병사, 공격, 이지중대, \n",
      "코미디: 사랑, 사람, 친구, 남자, 여자, 영화제, 가족, 마을, 사건, 이야기, 인생, 아이, 아버지, 마음, 모든, \n",
      "판타지: 사람, 사랑, 요괴, 영화제, 이야기, 소녀, 남자, 인간, 세상, 마을, 세계, 국제, 마법, 알렉스, 존재, \n"
     ]
    }
   ],
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d881e",
   "metadata": {},
   "source": [
    "#### LDA 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b41f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/decomposition/_lda.py:881: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(-1.0 * perword_bound)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00236818, 0.00236818, 0.00236818, ..., 0.00236819, 0.00236819,\n",
       "        0.00236819],\n",
       "       [0.00287849, 0.00287847, 0.00287847, ..., 0.00287849, 0.00287847,\n",
       "        0.00287848],\n",
       "       [0.00316709, 0.00316708, 0.00316709, ..., 0.00316712, 0.00316709,\n",
       "        0.00316709],\n",
       "       ...,\n",
       "       [0.00282587, 0.00282586, 0.00282585, ..., 0.00282588, 0.00282589,\n",
       "        0.00282587],\n",
       "       [0.00174136, 0.00174136, 0.00174136, ..., 0.00174136, 0.00174136,\n",
       "        0.00174136],\n",
       "       [0.00248986, 0.00248985, 0.00248986, ..., 0.00248987, 0.00248986,\n",
       "        0.00248986]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=50, \n",
    "                                      learning_method='online', \n",
    "                                      random_state=777, \n",
    "                                      max_iter=10,\n",
    "                                     doc_topic_prior=0.1)\n",
    "# 주제 간 중복 줄이기 위해 topic_word_prior 조정 가능\n",
    "# 문서 간 주제 중복 줄이기 위해 doc_topic_prior 조정 가능\n",
    "\n",
    "lda_model.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fe0024e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "케닝턴 강태 프란체스코 켄톤 얏타킹 야크 해갈 큰손 갈리아 저금 맥거핀 운수 담시 뒤안길 류승완\n",
      "Topic #1:\n",
      "지적 범생 퀜틴 쭈욱 발발 화련 프랭코 리벨 전수 재결합 뚝방파 개척지 추쉐이 자금지원 순찰차\n",
      "Topic #2:\n",
      "몽롱 회문 골치 음악회 조원 아네믹 연차 블랙리스트 강매 태평 롱쇼 생활사 폭포 광위 피앙세\n",
      "Topic #3:\n",
      "윌슨 스나이더 닝징 손혜윤 커즌스 전제정치 업친 웨폰 하일 박진 인공위성 감서 레위기 캬바레 아킴\n",
      "Topic #4:\n",
      "고래사냥 정권 워크맨 우주망원경 컵케이크 대한문 동학혁명 슬하 얌체 혼성 영혼 메린 노닥 딱지왕 김태우\n",
      "Topic #5:\n",
      "액셀 사다코 검거 왈도 통칭 붐바 그린북 여름날 닥터 성인영화 프린세스 고서점 택했던 기관총 당진시\n",
      "Topic #6:\n",
      "먼지 요코하마 수욱 자만 신장 엠마뉴엘 섹쉬궁 조혁 스테롤 카시라 숙련 뉴스메이커 슈퍼컴퓨터 냄비 냠걀사원\n",
      "Topic #7:\n",
      "베렛 캐리 피오 자살 총사령관 골프장 츄츄 정보기관 병태 인지도 멸치 크리슈나 관성 이윤호 할아범\n",
      "Topic #8:\n",
      "열망 호창 횡설수설 수혈 연구실 서브프라임 계심 직상 아마게돈 플라멩코 프레임 실종자 러스트 용케 인영\n",
      "Topic #9:\n",
      "영화제 국제 사람 단편 서울 사랑 가족 친구 남자 소녀 아이 대한 이야기 적의 디스플레이\n",
      "Topic #10:\n",
      "미술감독 새봄 맥켈웨이 치약 별탈 허드슨만 크림반도 잠수 타이스 선량 최정 뻔뻔 애하 하천 저격총\n",
      "Topic #11:\n",
      "해걸 신성모독 백도 격없 파타콘 제드 심청 계단 산아 빛낼 치환 색쉬함 미츠키 가부 사빈\n",
      "Topic #12:\n",
      "상륙작전 츠쿠시 상대편 길거리 홍역 리페 사신 종말 마스 클로저 메츠 쏙쏙 스파르티 렉시스 막스\n",
      "Topic #13:\n",
      "아모레 경제위기 도둑 경시청 김정학 랜드마크 장착 조폭 모노크롬 랩퍼 강시 통강 상대자 텡보체 결찰\n",
      "Topic #14:\n",
      "동국대학교 텟페 종진 천방지축 간극 마크 위업 룰로 연제욱 이슬람 키파 연예계 모음곡 워터마크 미켈란젤로\n",
      "Topic #15:\n",
      "사람 두루미 사망 스웩 남성 사생결단 중고 친구 스티그 수정구 볼트 오후 항우울제 똥주 르르\n",
      "Topic #16:\n",
      "사람 사건 보고 경찰 친구 이야기 심리 얼마만큼 윌리스 사도 수사 아내 교만 마기아 부닌\n",
      "Topic #17:\n",
      "배츠 폴란드인 독심술 동냥 와플 이상자 비키 메릴란드 핸슬 러브콜 블랑크 산문 햄슨 투캅스 박민영\n",
      "Topic #18:\n",
      "고구 불똥 파네 복학 회복 브릭스 랍스터 쟈오쥬 첫사랑 박순 디에고 어지럼증 신문지 뉴욕주 쇠사슬\n",
      "Topic #19:\n",
      "호의 주성수 중부 수사반장 미군정 컵밥 신혼여행 유시 화강암 여대생 대란 짓밟힌 사유리 건실 노어\n",
      "Topic #20:\n",
      "인칭 콜텍 레르 과로사 버논 공로 중략 조로 킴벌리 군함 사히브 다나카 화폐 세균 휘슬러\n",
      "Topic #21:\n",
      "갱단 권익 히트 악수 계교 단백질 눈길 반나절 듀리아 남존 무서움 프락푸르 고령 인크레더블 정희\n",
      "Topic #22:\n",
      "존속 마셸 안창호 일면 재곤 입체파 델우드 크랜쇼 차량 뚱보 트라이포스 면지 기어이 홀로코스트 지희\n",
      "Topic #23:\n",
      "평행이론 박만수 방송극 주식시장 개화 치가 히프 충북 현기 발명가 매그놀리아 백설공주 종북 베르사유 첼리스트\n",
      "Topic #24:\n",
      "사람 사랑 영화제 사건 친구 남자 이야기 국제 마을 모든 가족 아이 아버지 여자 남편\n",
      "Topic #25:\n",
      "옹달샘 형우 파르하디 김곡 깡패 부량 나이트메어 마르케 스디얀 보호령 레빌 질산 헤네시 야자수 슬픔\n",
      "Topic #26:\n",
      "일원론 정력 가델 홀랑 대량학살 웠던 사피 데페 단돈 도손 단언 용비 뒤뜰 더펄 실물\n",
      "Topic #27:\n",
      "비석 나루토 병찬 일소 강탈 중심 경준 쥬스 재호 체육복 크라프트 공원 아핏차퐁 헬마 다도\n",
      "Topic #28:\n",
      "하랴 네덜란드어 효순 효정 운남 버는데 평화협정 양어머니 로스엔젤레스 규합 디귿 단점 돈줄 브리타니 집요\n",
      "Topic #29:\n",
      "다혜 장범준 영념 불새 절찬 뉴트 윌리암스버그 제자리걸음 윤찬 숙성 빈촌 페일 암호 읍내 병무청\n",
      "Topic #30:\n",
      "산호 코토브 나부코 체취뿐 아이 스킨십 출신 싸이보그 더밂녀 익부 심험 덕성 속주 적하 엄마\n",
      "Topic #31:\n",
      "세드나 모독 법썩 수하인 핏빛 신명 대홍수 헬름 승제 미로비젼 진선 맑음 공원 메리트 흉기\n",
      "Topic #32:\n",
      "약물 잉엘스 열거 쿄우야 파탈 발기 마그마 프리밍거 군함 장대높이뛰기 협소 룸파 소셜미디어 외계인 황제펭귄\n",
      "Topic #33:\n",
      "로렌 또미 사내아이 건추 숨구멍 메이플 공중화장실 레저 풍선껌 반기 시마바라 더럼 소년교도소 호머 엘런\n",
      "Topic #34:\n",
      "기내 장안성 루트 케어 염하 레드와인 브래 뒷문 이주하 탠빌 황효명 봉알 감속 유리나 그린북\n",
      "Topic #35:\n",
      "북단 애니멀 르프 은행강도 승천 임마누엘 차사 증정 음율 켈빈 급매 르블랑 호랑이 동유럽 미노타우로스\n",
      "Topic #36:\n",
      "브라마 엮이 후안 홍익대 배척 뻔쩍 정글 종교 사카이 도덕 아핏차퐁 터스키기 덮치기 이야기 외따로\n",
      "Topic #37:\n",
      "딜리 트자 노동운동 퍼포머들 미카 공산주의 포인트 국악인 토핑 엘리자베스 김태일 남창 새벽같이 일장 국영수\n",
      "Topic #38:\n",
      "브록 와일스 잉베 보프 흥철 청자 에이리언 반토막 컬러 영리 폐쇄되어 인디애니페스트 처리 오거 조종\n",
      "Topic #39:\n",
      "정용화 이송희일 맥너슨 이기심 치요 레이더 비닐 아시아 베네딕 서은아 삐끼 교구 소스케 리에티 자그레브\n",
      "Topic #40:\n",
      "학질 하나라 해저 석호 장관 동명 버섯구름 촉발 안면 메인랜드 이감 삭발 만다린 기차역 달라\n",
      "Topic #41:\n",
      "풀착 베데스다 점정 천사이 코뮌 블루 바랜 문정 모턴 생존자 사고사 상호의존 돌파구 다홍 마이너스\n",
      "Topic #42:\n",
      "사다리 드라간 리드보컬 비벌리힐즈 무산 명나라 깊이 정율성 괴현상 출세작 루시엔 바르샤바 배분 번질 전통예술\n",
      "Topic #43:\n",
      "수목 테이블 대업 룡파 성냥개비 원금 캐츠킬 북유럽 마산창 테로 남제동 마허샬 소말리아 활화산 팔찌\n",
      "Topic #44:\n",
      "올드랭사 양재동 덤프트럭 안나 바오산 당주 디온드라 아즈텍 베벌리 딩후이 환상여행 뒹구 광고 비엘 즐라탄\n",
      "Topic #45:\n",
      "신부전증 케아 재래식 지단 칸나비스 타블론 인권영화제 노량진 교사 비포장도로 블랭크 자림 와이드 무뎌 텐더로\n",
      "Topic #46:\n",
      "관성 오른쪽 해법 폭격 미가 힌즈 양분 덴젤 시부야 달리아 사안 불로 인사이트 기독교도 민기\n",
      "Topic #47:\n",
      "애니메이션 국제 마을 영화제 아이 하느님 사람 서영 후점 타나 거절 서부 사랑 정은수 친구\n",
      "Topic #48:\n",
      "앵콜 철처 왜소 코더 허쉬 푸쇽 사울 병원체 택일 생화 아프리카인 태양 달걀 폭락 리광\n",
      "Topic #49:\n",
      "북미 티라노사우루스 내장 넷플릭스 고택 적응력 신공 베르히 북서쪽 놀롯 수반 현주 상가 합집 낭보\n",
      "\n",
      "Max Probability Topic for Each Document:\n",
      "Document #0:\n",
      "Max Probability Topic: 24, Probability: 0.8840\n",
      "\n",
      "Document #1:\n",
      "Max Probability Topic: 24, Probability: 0.8590\n",
      "\n",
      "Document #2:\n",
      "Max Probability Topic: 24, Probability: 0.8448\n",
      "\n",
      "Document #3:\n",
      "Max Probability Topic: 24, Probability: 0.8859\n",
      "\n",
      "Document #4:\n",
      "Max Probability Topic: 24, Probability: 0.8332\n",
      "\n",
      "Document #5:\n",
      "Max Probability Topic: 24, Probability: 0.8865\n",
      "\n",
      "Document #6:\n",
      "Max Probability Topic: 24, Probability: 0.9006\n",
      "\n",
      "Document #7:\n",
      "Max Probability Topic: 24, Probability: 0.8822\n",
      "\n",
      "Document #8:\n",
      "Max Probability Topic: 24, Probability: 0.8363\n",
      "\n",
      "Document #9:\n",
      "Max Probability Topic: 24, Probability: 0.8756\n",
      "\n",
      "Document #10:\n",
      "Max Probability Topic: 24, Probability: 0.8889\n",
      "\n",
      "Document #11:\n",
      "Max Probability Topic: 24, Probability: 0.8400\n",
      "\n",
      "Document #12:\n",
      "Max Probability Topic: 24, Probability: 0.7699\n",
      "\n",
      "Document #13:\n",
      "Max Probability Topic: 24, Probability: 0.8524\n",
      "\n",
      "Document #14:\n",
      "Max Probability Topic: 24, Probability: 0.8901\n",
      "\n",
      "Document #15:\n",
      "Max Probability Topic: 24, Probability: 0.8648\n",
      "\n",
      "Document #16:\n",
      "Max Probability Topic: 24, Probability: 0.9092\n",
      "\n",
      "Document #17:\n",
      "Max Probability Topic: 24, Probability: 0.8876\n",
      "\n",
      "Document #18:\n",
      "Max Probability Topic: 24, Probability: 0.8615\n",
      "\n",
      "Document #19:\n",
      "Max Probability Topic: 24, Probability: 0.9147\n",
      "\n",
      "Document #20:\n",
      "Max Probability Topic: 24, Probability: 0.8780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LDA의 결과 토픽과 각 단어의 비중을 출력\n",
    "n_top_words = 15\n",
    "tf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"Topics in LDA model:\")\n",
    "print_top_words(lda_model, tf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Max Probability Topic for Each Document:\")\n",
    "documents = genre\n",
    "max_topic_idxs = print_max_topic_for_documents(lda_model, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d08ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes=[]\n",
    "for i in range(len(genre_)):\n",
    "    attributes.append(get_topic_words(lda_model, max_topic_idxs[i], tf_feature_names, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f197e",
   "metadata": {},
   "source": [
    "## STEP 4. WEAT score 계산과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99e1b3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "# WEAT score function\n",
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "def weat_score(X, Y, A, B):    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "779347d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SF 가족 -0.62751734\n",
      "SF 공연 -0.31261918\n",
      "SF 공포(호러) -0.7071042\n",
      "SF 기타 0.053081196\n",
      "SF 다큐멘터리 0.3014637\n",
      "SF 드라마 -0.5334929\n",
      "SF 멜로로맨스 -0.8641359\n",
      "SF 뮤지컬 0.11361934\n",
      "SF 미스터리 -0.6645717\n",
      "SF 범죄 0.02021459\n",
      "SF 사극 -0.65355295\n",
      "SF 서부극(웨스턴) -0.38777822\n",
      "SF 성인물(에로) -0.6842467\n",
      "SF 스릴러 -0.54667616\n",
      "SF 애니메이션 0.16000181\n",
      "SF 액션 -0.12658557\n",
      "SF 어드벤처 -1.0643228\n",
      "SF 전쟁 0.38560516\n",
      "SF 코미디 -0.7912917\n",
      "SF 판타지 -0.44812104\n",
      "가족 공연 0.43599573\n",
      "가족 공포(호러) -0.018791296\n",
      "가족 기타 0.7808947\n",
      "가족 다큐멘터리 0.8369918\n",
      "가족 드라마 0.21816558\n",
      "가족 멜로로맨스 -0.76249176\n",
      "가족 뮤지컬 0.8899251\n",
      "가족 미스터리 -0.12492989\n",
      "가족 범죄 0.5542809\n",
      "가족 사극 0.28596336\n",
      "가족 서부극(웨스턴) 0.7796064\n",
      "가족 성인물(에로) -0.5068402\n",
      "가족 스릴러 -0.024650745\n",
      "가족 애니메이션 0.918894\n",
      "가족 액션 0.5403716\n",
      "가족 어드벤처 -0.32417694\n",
      "가족 전쟁 0.90272003\n",
      "가족 코미디 -0.25929192\n",
      "가족 판타지 0.5170888\n",
      "공연 공포(호러) -0.23789923\n",
      "공연 기타 0.69263273\n",
      "공연 다큐멘터리 0.7110423\n",
      "공연 드라마 -0.3330407\n",
      "공연 멜로로맨스 -0.7960861\n",
      "공연 뮤지컬 1.0654726\n",
      "공연 미스터리 -0.2888558\n",
      "공연 범죄 0.27542812\n",
      "공연 사극 -0.22270049\n",
      "공연 서부극(웨스턴) 0.056426007\n",
      "공연 성인물(에로) -0.65336853\n",
      "공연 스릴러 -0.22501819\n",
      "공연 애니메이션 0.78756833\n",
      "공연 액션 0.21122774\n",
      "공연 어드벤처 -0.49451336\n",
      "공연 전쟁 0.72735995\n",
      "공연 코미디 -0.4765761\n",
      "공연 판타지 0.08198553\n",
      "공포(호러) 기타 0.5039196\n",
      "공포(호러) 다큐멘터리 0.6250695\n",
      "공포(호러) 드라마 0.11175968\n",
      "공포(호러) 멜로로맨스 -0.79225695\n",
      "공포(호러) 뮤지컬 0.5388284\n",
      "공포(호러) 미스터리 -0.375096\n",
      "공포(호러) 범죄 0.88869053\n",
      "공포(호러) 사극 0.21218362\n",
      "공포(호러) 서부극(웨스턴) 0.4920058\n",
      "공포(호러) 성인물(에로) -0.48200658\n",
      "공포(호러) 스릴러 -0.020643936\n",
      "공포(호러) 애니메이션 0.58220786\n",
      "공포(호러) 액션 0.83863866\n",
      "공포(호러) 어드벤처 -0.27299258\n",
      "공포(호러) 전쟁 0.798498\n",
      "공포(호러) 코미디 -0.18379174\n",
      "공포(호러) 판타지 0.3943585\n",
      "기타 다큐멘터리 0.54768234\n",
      "기타 드라마 -0.9564021\n",
      "기타 멜로로맨스 -0.91526735\n",
      "기타 뮤지컬 0.24077673\n",
      "기타 미스터리 -0.5107133\n",
      "기타 범죄 -0.02389268\n",
      "기타 사극 -0.6162773\n",
      "기타 서부극(웨스턴) -0.38690522\n",
      "기타 성인물(에로) -0.8253986\n",
      "기타 스릴러 -0.44550776\n",
      "기타 애니메이션 0.28899685\n",
      "기타 액션 -0.117283314\n",
      "기타 어드벤처 -0.8303321\n",
      "기타 전쟁 0.2333408\n",
      "기타 코미디 -0.8179537\n",
      "기타 판타지 -0.55051017\n",
      "다큐멘터리 드라마 -0.93651116\n",
      "다큐멘터리 멜로로맨스 -0.9054507\n",
      "다큐멘터리 뮤지컬 -0.34971473\n",
      "다큐멘터리 미스터리 -0.61307484\n",
      "다큐멘터리 범죄 -0.17703015\n",
      "다큐멘터리 사극 -0.7355607\n",
      "다큐멘터리 서부극(웨스턴) -0.5387033\n",
      "다큐멘터리 성인물(에로) -0.8145532\n",
      "다큐멘터리 스릴러 -0.54072124\n",
      "다큐멘터리 애니메이션 -0.34304586\n",
      "다큐멘터리 액션 -0.28188878\n",
      "다큐멘터리 어드벤처 -0.96949905\n",
      "다큐멘터리 전쟁 -0.004730916\n",
      "다큐멘터리 코미디 -0.8797291\n",
      "다큐멘터리 판타지 -0.79422426\n",
      "드라마 멜로로맨스 -0.83198524\n",
      "드라마 뮤지컬 0.9558792\n",
      "드라마 미스터리 -0.19667071\n",
      "드라마 범죄 0.44358334\n",
      "드라마 사극 0.123415105\n",
      "드라마 서부극(웨스턴) 0.3968316\n",
      "드라마 성인물(에로) -0.62199515\n",
      "드라마 스릴러 -0.09989355\n",
      "드라마 애니메이션 0.963559\n",
      "드라마 액션 0.40032724\n",
      "드라마 어드벤처 -0.48126763\n",
      "드라마 전쟁 0.7505762\n",
      "드라마 코미디 -0.49764228\n",
      "드라마 판타지 0.5185705\n",
      "멜로로맨스 뮤지컬 0.93779606\n",
      "멜로로맨스 미스터리 0.7749666\n",
      "멜로로맨스 범죄 1.104842\n",
      "멜로로맨스 사극 0.8123967\n",
      "멜로로맨스 서부극(웨스턴) 0.9483666\n",
      "멜로로맨스 성인물(에로) 0.66618603\n",
      "멜로로맨스 스릴러 0.89406693\n",
      "멜로로맨스 애니메이션 0.90779334\n",
      "멜로로맨스 액션 1.0188837\n",
      "멜로로맨스 어드벤처 0.5881173\n",
      "멜로로맨스 전쟁 0.9832515\n",
      "멜로로맨스 코미디 0.82457966\n",
      "멜로로맨스 판타지 0.83036846\n",
      "뮤지컬 미스터리 -0.5431258\n",
      "뮤지컬 범죄 -0.06746075\n",
      "뮤지컬 사극 -0.7454874\n",
      "뮤지컬 서부극(웨스턴) -0.4814754\n",
      "뮤지컬 성인물(에로) -0.8439363\n",
      "뮤지컬 스릴러 -0.48707804\n",
      "뮤지컬 애니메이션 0.0954866\n",
      "뮤지컬 액션 -0.17160186\n",
      "뮤지컬 어드벤처 -0.8637109\n",
      "뮤지컬 전쟁 0.20170142\n",
      "뮤지컬 코미디 -0.8449655\n",
      "뮤지컬 판타지 -0.5956849\n",
      "미스터리 범죄 1.059674\n",
      "미스터리 사극 0.28203493\n",
      "미스터리 서부극(웨스턴) 0.50866336\n",
      "미스터리 성인물(에로) -0.41951028\n",
      "미스터리 스릴러 0.36214206\n",
      "미스터리 애니메이션 0.56951106\n",
      "미스터리 액션 0.89895236\n",
      "미스터리 어드벤처 -0.06568917\n",
      "미스터리 전쟁 0.7768718\n",
      "미스터리 코미디 0.0062668207\n",
      "미스터리 판타지 0.40809825\n",
      "범죄 사극 -0.48287123\n",
      "범죄 서부극(웨스턴) -0.387973\n",
      "범죄 성인물(에로) -0.87588066\n",
      "범죄 스릴러 -1.2568392\n",
      "범죄 애니메이션 0.08675785\n",
      "범죄 액션 -0.33658063\n",
      "범죄 어드벤처 -0.7090173\n",
      "범죄 전쟁 0.32007784\n",
      "범죄 코미디 -0.71285295\n",
      "범죄 판타지 -0.24508613\n",
      "사극 서부극(웨스턴) 0.38224122\n",
      "사극 성인물(에로) -0.58548206\n",
      "사극 스릴러 -0.1848437\n",
      "사극 애니메이션 0.7552726\n",
      "사극 액션 0.48928133\n",
      "사극 어드벤처 -0.6267736\n",
      "사극 전쟁 1.0075097\n",
      "사극 코미디 -0.5144786\n",
      "사극 판타지 0.35908407\n",
      "서부극(웨스턴) 성인물(에로) -0.7105474\n",
      "서부극(웨스턴) 스릴러 -0.4148307\n",
      "서부극(웨스턴) 애니메이션 0.5331171\n",
      "서부극(웨스턴) 액션 0.31630576\n",
      "서부극(웨스턴) 어드벤처 -0.84139436\n",
      "서부극(웨스턴) 전쟁 0.84540945\n",
      "서부극(웨스턴) 코미디 -0.80164677\n",
      "서부극(웨스턴) 판타지 0.029170379\n",
      "성인물(에로) 스릴러 0.52687544\n",
      "성인물(에로) 애니메이션 0.80076396\n",
      "성인물(에로) 액션 0.7710026\n",
      "성인물(에로) 어드벤처 0.326301\n",
      "성인물(에로) 전쟁 0.84107345\n",
      "성인물(에로) 코미디 0.4925544\n",
      "성인물(에로) 판타지 0.65244603\n",
      "스릴러 애니메이션 0.5039762\n",
      "스릴러 액션 0.9193686\n",
      "스릴러 어드벤처 -0.16814633\n",
      "스릴러 전쟁 0.76733917\n",
      "스릴러 코미디 -0.12245046\n",
      "스릴러 판타지 0.30223578\n",
      "애니메이션 액션 -0.19472349\n",
      "애니메이션 어드벤처 -0.9440722\n",
      "애니메이션 전쟁 0.17327265\n",
      "애니메이션 코미디 -0.87909395\n",
      "애니메이션 판타지 -0.7686405\n",
      "액션 어드벤처 -0.7660952\n",
      "액션 전쟁 0.5728499\n",
      "액션 코미디 -0.71054685\n",
      "액션 판타지 -0.17839108\n",
      "어드벤처 전쟁 0.9520992\n",
      "어드벤처 코미디 0.1729432\n",
      "어드벤처 판타지 0.9980255\n",
      "전쟁 코미디 -0.8829301\n",
      "전쟁 판타지 -0.6076048\n",
      "코미디 판타지 0.78195417\n"
     ]
    }
   ],
   "source": [
    "# embedding model과 단어 셋으로 WEAT score 구해보기\n",
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)\n",
    "        \n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4713d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_heatmap(matrix, genre_name, title, save=True):\n",
    "    np.random.seed(0)\n",
    "    # 한글 지원 폰트\n",
    "    sns.set(font='NanumGothic')\n",
    "\n",
    "    # 마이너스 부호 \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,13))\n",
    "    sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True, cmap='RdYlGn_r', ax=ax)\n",
    "\n",
    "    ax.set_title(title, fontsize=20, pad=20)\n",
    "    if save:\n",
    "        plt.savefig('heatmap_'+title+'.png', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_heatmap(matrix, genre_name, 'LDA', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
