{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00765b8",
   "metadata": {},
   "source": [
    "## STEP 1. 형태소 분석기를 이용하여 품사에 따라 해당 단어를 추출하기\n",
    "\n",
    "- 명사만 추출 `nouns.txt`\n",
    "- 명사, 동사 추출 `nouns_verbs.txt`\n",
    "- 명사, 동사, 형용사 추출 `nouns_verbs_adjectives.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "221c8172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(input_file_path, \\'r\\', encoding=\\'utf-8\\') as rf:\\n    with open(output_file_path, \\'w\\', encoding=\\'utf-8\\') as wf:\\n        lines = rf.readlines()\\n        for line in tqdm(lines):\\n            words = okt.pos(line, stem=True, norm=True)\\n            res = [w[0] for w in words if w[1] == \"Noun\" or w[1] == \"Verb\"]\\n            wf.write(\\' \\'.join(res) + \\'\\n\\')\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시간이 오래 소요되므로 파일로 저장\n",
    "# word2vec 생성을 위한 토큰화\n",
    "import os\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "okt = Okt()\n",
    "input_file_path = os.getenv('HOME') + '/aiffel/weat/synopsis.txt'\n",
    "output_file_path = './tokenized/nouns.txt'\n",
    "\n",
    "'''\n",
    "with open(input_file_path, 'r', encoding='utf-8') as rf:\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as wf:\n",
    "        lines = rf.readlines()\n",
    "        for line in tqdm(lines):\n",
    "            words = okt.pos(line, stem=True, norm=True)\n",
    "            res = [w[0] for w in words if w[1] == \"Noun\" or w[1] == \"Verb\"]\n",
    "            wf.write(' '.join(res) + '\\n')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2bfc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장해놓은 토큰 파일 불러오기\n",
    "tokenized=[]\n",
    "with open(output_file_path, 'r', encoding='utf-8') as rf:\n",
    "    for line in rf:\n",
    "        # 각 줄을 읽어 공백을 기준으로 단어를 나누고 리스트로 변환\n",
    "        words = line.strip().split()\n",
    "        tokenized.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b119bd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1342179\n"
     ]
    }
   ],
   "source": [
    "# 개수 세기\n",
    "cnt=0\n",
    "for words in tokenized:\n",
    "    cnt+=len(words)\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e403ee",
   "metadata": {},
   "source": [
    "- Noun 개수: 1342179\n",
    "- Noun & Verb 개수: 1723183\n",
    "- Noun & Verb & Adjective 개수: 1850315"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00b307",
   "metadata": {},
   "source": [
    "## STEP 2. 추출된 결과로 embedding model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d870d918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('작품', 0.8968769907951355),\n",
       " ('다큐멘터리', 0.8536402583122253),\n",
       " ('영화로', 0.8201987743377686),\n",
       " ('드라마', 0.8191244006156921),\n",
       " ('형식', 0.7980725169181824),\n",
       " ('주제', 0.7934190630912781),\n",
       " ('코미디', 0.7801860570907593),\n",
       " ('감동', 0.7790406942367554),\n",
       " ('소재', 0.7768155336380005),\n",
       " ('인터뷰', 0.7672218084335327)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94342541",
   "metadata": {},
   "source": [
    "## STEP 3. target, attribute 단어 셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10b5e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 토큰화 파일 저장 함수\n",
    "def save_tokens(input_file_name, output_file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+input_file_name, 'r') as fread: \n",
    "        with open('./tokenized/'+output_file_name, 'w', encoding='utf-8') as fwrite:\n",
    "            print(input_file_name, '파일을 읽고 있습니다.')\n",
    "            lines = fread.readlines()\n",
    "            for line in tqdm(lines):\n",
    "                words = okt.pos(line, stem=True, norm=True)\n",
    "                res = [w[0] for w in words if w[1] == \"Noun\" or w[1] == \"Verb\"]\n",
    "                fwrite.write(' '.join(res) + '\\n')\n",
    "# 토큰 파일 불러오는 함수              \n",
    "def read_tokens(file_name):\n",
    "    with open('./tokenized/' + file_name, 'r', encoding='utf-8') as fread:\n",
    "        # 파일 전체를 읽어서 줄바꿈 문자를 제거하고 하나의 문자열로 반환\n",
    "        tokenized = fread.read().replace('\\n', ' ')\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2303e27d",
   "metadata": {},
   "source": [
    "명사만 추출할 때에는 `okt.nouns()`을 활용해보았지만 소요 시간 차이 크지 않았음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73aeaa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['하다', '되다', '그녀', '자신', '위해', '않다', '이다', '되어다', '싶다', '점점', '때문', '오다', '가다', '과연', '이제', '통해']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 처리 함수\n",
    "with open('./stopwords.txt', 'r', encoding='utf-8') as fread:\n",
    "    stopwords = [line.strip() for line in fread.readlines()]\n",
    "    print(stopwords)\n",
    "    \n",
    "def remove_stopwords(tokenized, stopwords):\n",
    "    words = tokenized.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    result = ' '.join(filtered_words)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de93a325",
   "metadata": {},
   "source": [
    "stopwords 구성 기준: 모든 데이터셋에서 자주 등장하고, 주제와 관련 없는 단어 선정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc9e03b",
   "metadata": {},
   "source": [
    "### target 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b76f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 데이터 불러오기\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "# 품사에 따라 바꾸기\n",
    "output_art_txt = 'synopsis_art_nouns.txt'\n",
    "output_gen_txt = 'synopsis_gen_nouns.txt'\n",
    "\n",
    "'''\n",
    "save_tokens(art_txt, output_art_txt)\n",
    "save_tokens(gen_txt, output_gen_txt)\n",
    "'''\n",
    "\n",
    "art=read_tokens(output_art_txt)\n",
    "gen=read_tokens(output_gen_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a19d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "art=remove_stopwords(art, stopwords)\n",
    "gen=remove_stopwords(gen, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ded6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_type = 'count'\n",
    "\n",
    "# TF-IDF\n",
    "if vec_type=='tfidf':\n",
    "    vectorizer = TfidfVectorizer()\n",
    "# TF\n",
    "elif vec_type=='count':\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "X = vectorizer.fit_transform([art,gen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b541577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예술영화를 대표하는 단어들:\n",
      "시작, 사랑, 사람, 영화, 친구, 남자, 가족, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이야기, 마을, 사건, 마음, 세상, 아버지, 아이, 엄마, 모든, 여자, 대한, 서로, 시간, 다시, 아들, 소녀, 아내, 다른, 영화제, 사이, 세계, 사실, 하나, 남편, 감독, 여행, 인생, 발견, 모두, 순간, 우리, 가장, 마지막, 아빠, 생활, 모습, 기억, 죽음, 비밀, 학교, 음악, 한편, 소년, 생각, 도시, 명의, 결혼, 사고, 전쟁, 위기, 최고, 이자, 과거, 일상, 경찰, 간다, 상황, 미국, 운명, 결심, 관계, 현실, 지금, 단편, 여인, 하루, 이름, 이후, 준비, 인간, 만난, 감정, 처음, 국제, 누구, 살인, 충격, 동안, 존재, 그린, 어머니, 연인, 계속, 동생, 작품, 한국, 청년, 가지, 상처, 할머니, 목숨, 이상, 희망, \n",
      "\n",
      "일반영화를 대표하는 단어들:\n",
      "영화제, 사람, 시작, 국제, 영화, 친구, 사랑, 남자, 이야기, 대한, 서울, 여자, 사건, 남편, 아이, 가족, 아버지, 다른, 마을, 시간, 엄마, 아들, 모든, 단편, 마음, 사실, 다시, 세계, 모습, 작품, 생각, 서로, 세상, 발견, 감독, 아내, 관계, 소녀, 사이, 하나, 우리, 애니메이션, 여성, 죽음, 인간, 생활, 한편, 결혼, 상황, 모두, 기억, 명의, 소년, 여행, 가장, 간다, 순간, 도시, 비밀, 학교, 과거, 가지, 이자, 경찰, 마지막, 미국, 동안, 전쟁, 주인공, 대해, 존재, 현실, 연출, 사고, 살인, 일상, 어머니, 계속, 사회, 인생, 다큐멘터리, 부문, 섹스, 최고, 바로, 의도, 동생, 하루, 위기, 계획, 정체, 한국, 이후, 조직, 회사, 보고, 부산, 아빠, 부부, 일본, "
     ]
    }
   ],
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "w1_, w2_ = [], []\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    word = vectorizer.get_feature_names()[w1[i][0]]\n",
    "    print(word, end=', ')\n",
    "    w1_.append(word)\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    word = vectorizer.get_feature_names()[w2[i][0]]\n",
    "    print(word, end=', ')\n",
    "    w2_.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe162c1",
   "metadata": {},
   "source": [
    "> Q. 왜 두 문서 사이에 중복값이 많을까?  \n",
    "> A. 둘 다 영화를 다루고 있으므로 공통된 어휘가 많이 사용됨. 또한 현재 문서가 2개만 존재하므로 단어 간의 IDF 값 차이가 적어져 TF 값이 TF-IDF 값에 영향을 끼치는 정도가 커짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9bd4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "n = 15\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6cbfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예술영화를 대표하는 단어들:\n",
      "음악, 운명, 결심, 지금, 여인, 이름, 준비, 만난, 감정, 처음, 누구, 충격, 그린, 연인, 청년, \n",
      "\n",
      "일반영화를 대표하는 단어들:\n",
      "서울, 애니메이션, 여성, 주인공, 대해, 연출, 사회, 다큐멘터리, 부문, 섹스, 바로, 의도, 계획, 정체, 조직, "
     ]
    }
   ],
   "source": [
    "print('예술영화를 대표하는 단어들:')\n",
    "for word in target_art:\n",
    "    print(word, end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for word in target_gen:\n",
    "    print(word, end=', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db21b57",
   "metadata": {},
   "source": [
    "### attribute 단어 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a7c7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#파일 불러와서 TF-IDF 생성\n",
    "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
    "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
    "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
    "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
    "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
    "genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
    "         '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지']\n",
    "# 품사에 따라 바꾸기\n",
    "output_genre_txt = [txt.strip('.txt')+'_nouns.txt' for txt in genre_txt]\n",
    "\n",
    "genre = []\n",
    "for file_name, output_file_name in zip(genre_txt, output_genre_txt):\n",
    "    #save_tokens(file_name, output_file_name)\n",
    "    genre.append(read_tokens(output_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3a8a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "genre_ = []\n",
    "for gen in genre:\n",
    "    genre_.append(remove_stopwords(gen, stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f61599c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9f6bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SF: 지구, 시작, 사람, 인류, 인간, 미래, 우주, 로봇, 세계, 모든, 박사, 우주선, 외계, 존재, 세상, \n",
      "가족: 엄마, 아빠, 가족, 영화제, 친구, 아주르, 아버지, 시작, 아들, 마을, 국제, 낙타, 할머니, 씨제이, 동구, \n",
      "공연: 오페라, 사랑, 토스카, 실황, 올레, 카바, 공연, 오텔로, 리골레토, 백작, 프레, 베르디, 카르피, 비바, 왕자, \n",
      "공포(호러): 시작, 사람, 친구, 사건, 공포, 발견, 죽음, 마을, 가족, 악령, 남자, 좀비, 영화, 사실, 소녀, \n",
      "기타: 영화제, 국제, 서울, 단편, 영화, 사람, 이야기, 남자, 시작, 사랑, 뉴미디어, 페스티벌, 여자, 대한, 독립, \n",
      "다큐멘터리: 영화제, 영화, 다큐, 국제, 다큐멘터리, 사람, 이야기, 대한, 감독, 서울, 우리, 시작, 세계, 여성, 가족, \n",
      "드라마: 영화제, 사람, 사랑, 영화, 시작, 국제, 남자, 친구, 이야기, 엄마, 여자, 아버지, 가족, 단편, 서울, \n",
      "멜로로맨스: 사랑, 시작, 남편, 남자, 여자, 사람, 친구, 섹스, 마음, 결혼, 서로, 아내, 관계, 부부, 엄마, \n",
      "뮤지컬: 뮤지컬, 사랑, 에스메랄다, 음악, 충무로, 모차르트, 영화, 토스카, 니웨, 카바, 영화제, 바흐, 페뷔스, 프롤, 모도, \n",
      "미스터리: 사건, 시작, 사람, 발견, 사고, 진실, 죽음, 기억, 살인, 친구, 아내, 남자, 아이, 민혁, 사실, \n",
      "범죄: 사건, 경찰, 시작, 범죄, 조직, 살인, 사람, 마약, 형사, 남자, 모든, 살해, 수사, 발견, 한길수, \n",
      "사극: 조선, 시작, 신기전, 사랑, 아가멤논, 황제, 루안, 최고, 운명, 사람, 하선, 전쟁, 윤서, 트로이, 세자, \n",
      "서부극(웨스턴): 서부, 보안관, 벌린, 카우보이, 그레이프바인, 헨리, 마을, 개릿, 아이, 시작, 무법자, 프린트, 마적, 태구, 현상금, \n",
      "성인물(에로): 남편, 마사지, 섹스, 관계, 영화, 정사, 남자, 시작, 여자, 유부녀, 마음, 사랑, 에피소드, 그린, 아내, \n",
      "스릴러: 사건, 시작, 사람, 살인, 남자, 발견, 아내, 경찰, 친구, 모든, 사실, 살해, 가족, 형사, 비밀, \n",
      "애니메이션: 애니메이션, 국제, 영화제, 친구, 인디애니페스트, 시작, 사람, 페스티벌, 서울, 이야기, 아이, 마을, 소녀, 세계, 세상, \n",
      "액션: 시작, 조직, 사건, 사람, 경찰, 전쟁, 모든, 목숨, 사실, 친구, 가족, 요원, 임무, 범죄, 세계, \n",
      "어드벤처: 시작, 친구, 마을, 아버지, 영화, 아이, 사람, 여행, 세계, 앤트, 세상, 가족, 모험, 비밀, 대한, \n",
      "전쟁: 전쟁, 독일군, 전투, 작전, 시작, 부대, 윈터스, 독일, 연합군, 미군, 임무, 사람, 나치, 병사, 공격, \n",
      "코미디: 시작, 사랑, 사람, 친구, 영화, 남자, 여자, 영화제, 가족, 마을, 사건, 이야기, 인생, 아이, 아버지, \n",
      "판타지: 시작, 사람, 사랑, 요괴, 영화제, 이야기, 영화, 소녀, 남자, 인간, 세상, 마을, 세계, 국제, 마법, \n"
     ]
    }
   ],
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5700a6",
   "metadata": {},
   "source": [
    "## STEP 4. WEAT score 계산과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f366d71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "# WEAT score function\n",
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "def weat_score(X, Y, A, B):    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbdc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding model과 단어 셋으로 WEAT score 구해보기\n",
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)\n",
    "        \n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_heatmap(matrix, genre_name, title, save=True):\n",
    "    np.random.seed(0)\n",
    "    # 한글 지원 폰트\n",
    "    sns.set(font='NanumGothic')\n",
    "\n",
    "    # 마이너스 부호 \n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,13))\n",
    "    sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True, cmap='RdYlGn_r', ax=ax)\n",
    "\n",
    "    ax.set_title(title, fontsize=20, pad=20)\n",
    "    if save:\n",
    "        plt.savefig('heatmap_'+title+'.png', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_heatmap(matrix, genre_name, 'simple_tfidf', True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
