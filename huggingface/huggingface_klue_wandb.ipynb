{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5835d11",
   "metadata": {},
   "source": [
    "# HuggingFace 커스텀 프로젝트\n",
    "- model(klue/ber-base)를 활용하여 NSMC(Naver Sentiment Movie Corpus) task 수행\n",
    "- 데이터: https://github.com/e9t/nsmc\n",
    "- model: https://huggingface.co/klue/bert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "919ae914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(np.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8af05",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e397865b",
   "metadata": {},
   "source": [
    "### Huggingface dataset에서 불러오기\n",
    "NSMC 데이터셋 https://huggingface.co/datasets/Blpeng/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3b6e504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Blpeng___nsmc-55757a98c8abea78\n",
      "Reusing dataset csv (/aiffel/.cache/huggingface/datasets/csv/Blpeng___nsmc-55757a98c8abea78/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f5632777b84d1b88ba284df038be04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Blpeng/nsmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e649a3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'id', 'document', 'label'],\n",
       "        num_rows: 400000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378aa284",
   "metadata": {},
   "source": [
    "train 데이터만 존재 -> 데이터 분할 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd692155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0 : 0\n",
      "id : 8112052\n",
      "document : 어릴때보고 지금다시봐도 재밌어요ㅋㅋ\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 1\n",
      "id : 8132799\n",
      "document : 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 2\n",
      "id : 4655635\n",
      "document : 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 3\n",
      "id : 9251303\n",
      "document : 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 4\n",
      "id : 10067386\n",
      "document : 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = ds['train']\n",
    "cols = train.column_names\n",
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8078ef9",
   "metadata": {},
   "source": [
    "### 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcb72498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/csv/Blpeng___nsmc-55757a98c8abea78/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-7ae5fa8d6b787a51.arrow\n"
     ]
    }
   ],
   "source": [
    "# 결측치를 제거하는 함수 정의\n",
    "def remove_missing_values(data):\n",
    "    # 모든 값이 존재하는지 확인\n",
    "    return all(value is not None for value in data.values())\n",
    "\n",
    "# train, test 데이터셋에서 결측치 제거\n",
    "dataset = train.filter(remove_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5239b6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'id', 'document', 'label'],\n",
       "    num_rows: 399984\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f5eaf",
   "metadata": {},
   "source": [
    "약 16개의 데이터 제거됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9967d20",
   "metadata": {},
   "source": [
    "> Q. tokeinzer에 전처리 과정도 포함되어 있는 거 아닌가? 왜 결측치가 있는 데이터는 못 받지?  \n",
    "A. 텍스트 데이터를 모델이 이해할 수 있는 형식으로 변환하는 작업은 가능하지만, 결측치 제거와 같은 데이터 전처리 과정은 포함되어 있지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67635723",
   "metadata": {},
   "source": [
    "## Tokenizer & Model\n",
    "klue/ber-base 모델 https://huggingface.co/klue/bert-base  \n",
    "Auto Classes 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eede7b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b765d",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7bf750b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842aeb931059412693f64977d19b7f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/399984 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,    # binary classificaiton task에는 필요 없음\n",
    "        )\n",
    "\n",
    "tokenized_dataset = dataset.map(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1716823f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'id', 'document', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 399984\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db88490",
   "metadata": {},
   "source": [
    "### 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03b1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validtest = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "valid_test = train_validtest['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "train_data = train_validtest[\"train\"]\n",
    "valid_data = valid_test[\"train\"]\n",
    "test_data = valid_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1a6892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'id', 'document', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 319987\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c41cf",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39742714",
   "metadata": {},
   "source": [
    "### 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d06ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 관련 설정을 미리 지정\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = './outputs'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                             # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 16,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 1,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88f861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# binary classification\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a90e7c3",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb339cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5efb47dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "WandbCallback requires wandb to be installed. Run `pip install wandb`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_104/1094681830.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m# 학습시킬 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_arguments\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m# TrainingArguments을 통해 설정한 arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# evaluation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mdefault_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEFAULT_CALLBACKS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_callbacks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdefault_callbacks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         self.callback_handler = CallbackHandler(\n\u001b[0m\u001b[1;32m    394\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, callbacks, model, tokenizer, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36madd_callback\u001b[0;34m(self, callback)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mcb_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/integrations.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mhas_wandb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_wandb_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mhas_wandb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"WandbCallback requires wandb to be installed. Run `pip install wandb`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: WandbCallback requires wandb to be installed. Run `pip install wandb`."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=train_data,    # training dataset\n",
    "    eval_dataset=valid_data,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f9d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15618c",
   "metadata": {},
   "source": [
    "## wandb와 함께 사용\n",
    "- https://docs.wandb.ai/guides/integrations/huggingface\n",
    "- https://wandb.ai/matt24/vit-snacks-sweeps/reports/Hyperparameter-Search-for-HuggingFace-Transformer-Models--VmlldzoyMTUxNTg0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f526cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.9/site-packages (0.17.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.9/site-packages (from wandb) (2.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.9/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb) (4.0.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.19)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d66273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b32341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "# hyperparameters\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "        'values': [1, 2]\n",
    "        },\n",
    "    'batch_size': {\n",
    "        'values': [8, 16, 32]\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        'distribution': 'log_uniform_values',\n",
    "        'min': 1e-5,\n",
    "        'max': 1e-3\n",
    "    },\n",
    "    'weight_decay': {\n",
    "        'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d498b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        wandb_training_arguments = TrainingArguments(\n",
    "            output_dir='./sweeps',                             # output이 저장될 경로\n",
    "            evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "            learning_rate = config.learning_rate,                         #learning_rate\n",
    "            per_device_train_batch_size = config.batch_size,   # 각 device 당 batch size\n",
    "            per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "            num_train_epochs = config.epochs,                     # train 시킬 총 epochs\n",
    "            weight_decay = config.weight_decay,                        # weight decay\n",
    "            report_to=\"wandb\",  # enable logging to W&B\n",
    "            logging_steps=1,  # how often to log to W&B\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,           # 학습시킬 model\n",
    "            args=wandb_training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "            train_dataset=train_data,    # training dataset\n",
    "            eval_dataset=valid_data,       # evaluation dataset\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='nsmc_klue')\n",
    "\n",
    "wandb.agent(sweep_id, train, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=\"nsmc_klue\") as run:\n",
    "    # Pass the name and version of Artifact\n",
    "    my_model_name = \"<run_name>:latest\"\n",
    "    my_model_artifact = run.use_artifact(my_model_name)\n",
    "\n",
    "    # Download model weights to a folder and return the path\n",
    "    model_dir = my_model_artifact.download()\n",
    "\n",
    "    # Load your Hugging Face model from that folder\n",
    "    #  using the same model class\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dir, num_labels=num_labels\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
