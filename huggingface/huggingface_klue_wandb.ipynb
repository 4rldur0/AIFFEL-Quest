{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929a08c0",
   "metadata": {},
   "source": [
    "# HuggingFace 커스텀 프로젝트\n",
    "- model(klue/ber-base)를 활용하여 NSMC(Naver Sentiment Movie Corpus) task 수행\n",
    "- 데이터: https://github.com/e9t/nsmc\n",
    "- model: https://huggingface.co/klue/bert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bc72fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(np.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd6d15",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0282a2",
   "metadata": {},
   "source": [
    "### Huggingface dataset에서 불러오기\n",
    "NSMC 데이터셋 https://huggingface.co/datasets/Blpeng/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c158fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Blpeng___nsmc-55757a98c8abea78\n",
      "Reusing dataset csv (/aiffel/.cache/huggingface/datasets/csv/Blpeng___nsmc-55757a98c8abea78/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f204b63d7514b7e957719c2737d3cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'id', 'document', 'label'],\n",
       "        num_rows: 400000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Blpeng/nsmc\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2af035",
   "metadata": {},
   "source": [
    "train 데이터만 존재 -> 데이터 분할 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45de3bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0 : 0\n",
      "id : 8112052\n",
      "document : 어릴때보고 지금다시봐도 재밌어요ㅋㅋ\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 1\n",
      "id : 8132799\n",
      "document : 디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업이 부러웠는데. 사실 우리나라에서도 그 어려운시절에 끝까지 열정을 지킨 노라노 같은 전통이있어 저와 같은 사람들이 꿈을 꾸고 이뤄나갈 수 있다는 것에 감사합니다.\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 2\n",
      "id : 4655635\n",
      "document : 폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 3\n",
      "id : 9251303\n",
      "document : 와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런게 진짜 영화지\n",
      "label : 1\n",
      "\n",
      "\n",
      "Unnamed: 0 : 4\n",
      "id : 10067386\n",
      "document : 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train = ds['train']\n",
    "cols = train.column_names\n",
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aab939",
   "metadata": {},
   "source": [
    "### 불필요한 열 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1507d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'label'],\n",
       "    num_rows: 400000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds['train']\n",
    "ds = ds.remove_columns(['Unnamed: 0', 'id'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302c5b6",
   "metadata": {},
   "source": [
    "### 데이터셋 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a69d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /aiffel/.cache/huggingface/datasets/csv/Blpeng___nsmc-55757a98c8abea78/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-4b7488257526ee44.arrow\n"
     ]
    }
   ],
   "source": [
    "small_ds = ds.shuffle(seed=42).select(range(int(0.1 * len(ds))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88720ea3",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3aa80",
   "metadata": {},
   "source": [
    "### 결측치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd4eb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/csv/Blpeng___nsmc-55757a98c8abea78/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-c3384dd26d1364fe.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'label'],\n",
       "    num_rows: 39996\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치를 제거하는 함수 정의\n",
    "def remove_missing_values(data):\n",
    "    # 모든 값이 존재하는지 확인\n",
    "    return all(value is not None for value in data.values())\n",
    "\n",
    "# train, test 데이터셋에서 결측치 제거\n",
    "dataset = small_ds.filter(remove_missing_values)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236df96",
   "metadata": {},
   "source": [
    "약 16개의 데이터 제거됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0031d",
   "metadata": {},
   "source": [
    "> Q. tokeinzer에 전처리 과정도 포함되어 있는 거 아닌가? 왜 결측치가 있는 데이터는 못 받지?  \n",
    "A. 텍스트 데이터를 모델이 이해할 수 있는 형식으로 변환하는 작업은 가능하지만, 결측치 제거와 같은 데이터 전처리 과정은 포함되어 있지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8a6e7c",
   "metadata": {},
   "source": [
    "## Tokenizer & Model\n",
    "klue/ber-base 모델 https://huggingface.co/klue/bert-base  \n",
    "Auto Classes 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e736915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6891547",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66a83110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/csv/Blpeng___nsmc-55757a98c8abea78/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-853cf1e1ad9f6aa7.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'document', 'input_ids', 'label'],\n",
       "    num_rows: 39996\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,    # binary classificaiton task에는 필요 없음\n",
    "        )\n",
    "\n",
    "tokenized_dataset = dataset.map(transform, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10a59978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label'],\n",
       "    num_rows: 39996\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요없는 열 삭제\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['document'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09424d5",
   "metadata": {},
   "source": [
    "### 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1991747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validtest = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "valid_test = train_validtest['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "train_data = train_validtest[\"train\"]\n",
    "valid_data = valid_test[\"train\"]\n",
    "test_data = valid_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45b1a753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label'],\n",
       "    num_rows: 31996\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d6e4f",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1297ca0",
   "metadata": {},
   "source": [
    "### 학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "833c17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "# binary classification\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf2701",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e189b",
   "metadata": {},
   "source": [
    "#### wandb와 함께 사용\n",
    "- https://docs.wandb.ai/guides/integrations/huggingface\n",
    "- https://wandb.ai/matt24/vit-snacks-sweeps/reports/Hyperparameter-Search-for-HuggingFace-Transformer-Models--VmlldzoyMTUxNTg0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bc7ffcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.17.4-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "     |████████████████████████████████| 6.9 MB 4.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.15.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (3.19.1)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.9/site-packages (from wandb) (2.4.0)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "     |████████████████████████████████| 207 kB 71.6 MB/s            \n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-2.10.0-py2.py3-none-any.whl (302 kB)\n",
      "     |████████████████████████████████| 302 kB 68.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from wandb) (4.0.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from wandb) (59.4.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     |████████████████████████████████| 62 kB 2.3 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.7)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "     |████████████████████████████████| 143 kB 79.3 MB/s            \n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, urllib3, gitdb, setproctitle, sentry-sdk, gitpython, docker-pycreds, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.7\n",
      "    Uninstalling urllib3-1.26.7:\n",
      "      Successfully uninstalled urllib3-1.26.7\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.10.0 setproctitle-1.3.3 smmap-5.0.1 urllib3-1.26.19 wandb-0.17.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "248c80e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /aiffel/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00acc561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "# hyperparameters\n",
    "parameters_dict = {\n",
    "    'epochs': {\n",
    "        'values': [1, 2]\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        'distribution': 'log_uniform_values',\n",
    "        'min': 1e-5,\n",
    "        'max': 7e-5\n",
    "    },\n",
    "    'weight_decay': {\n",
    "        'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11cc778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        wandb_training_arguments = TrainingArguments(\n",
    "            fp16=True,\n",
    "            output_dir='./sweeps',                             # output이 저장될 경로\n",
    "            evaluation_strategy=\"steps\",             # evaluation 하는 빈도\n",
    "            eval_steps=1000,                         # 1000 스텝마다 평가 수행\n",
    "            learning_rate = config.learning_rate,                         #learning_rate\n",
    "            per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "            per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "            num_train_epochs = 1,                     # train 시킬 총 epochs\n",
    "            weight_decay = config.weight_decay,                        # weight decay\n",
    "            report_to=\"wandb\",  # enable logging to W&B\n",
    "            logging_steps=1,  # how often to log to W&B\n",
    "        )\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,           # 학습시킬 model\n",
    "            args=wandb_training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "            train_dataset=train_data,    # training dataset\n",
    "            eval_dataset=valid_data,       # evaluation dataset\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a148392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 캐시 지우기\n",
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c6a803e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8cvin4xn\n",
      "Sweep URL: https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: avx5fdkm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.0514280895740115e-06\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/AIFFEL-Quest/huggingface/wandb/run-20240719_055419-avx5fdkm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/4-rldur0/nsmc_klue/runs/avx5fdkm' target=\"_blank\">earthy-sweep-1</a></strong> to <a href='https://wandb.ai/4-rldur0/nsmc_klue' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/4-rldur0/nsmc_klue' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/4-rldur0/nsmc_klue/runs/avx5fdkm' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/runs/avx5fdkm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 31996\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 1:08:22, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.612400</td>\n",
       "      <td>0.371049</td>\n",
       "      <td>0.839750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.703700</td>\n",
       "      <td>0.359285</td>\n",
       "      <td>0.852250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.349990</td>\n",
       "      <td>0.855000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.337643</td>\n",
       "      <td>0.862250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.266700</td>\n",
       "      <td>0.342916</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.351580</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.693800</td>\n",
       "      <td>0.344790</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.342576</td>\n",
       "      <td>0.863500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./sweeps/checkpoint-500\n",
      "Configuration saved in ./sweeps/checkpoint-500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-1000\n",
      "Configuration saved in ./sweeps/checkpoint-1000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to ./sweeps/checkpoint-1500\n",
      "Configuration saved in ./sweeps/checkpoint-1500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-1500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-2000\n",
      "Configuration saved in ./sweeps/checkpoint-2000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to ./sweeps/checkpoint-2500\n",
      "Configuration saved in ./sweeps/checkpoint-2500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-2500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-3000\n",
      "Configuration saved in ./sweeps/checkpoint-3000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to ./sweeps/checkpoint-3500\n",
      "Configuration saved in ./sweeps/checkpoint-3500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-3500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-4000\n",
      "Configuration saved in ./sweeps/checkpoint-4000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-4000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./sweeps/checkpoint-4500\n",
      "Configuration saved in ./sweeps/checkpoint-4500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-4500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-5000\n",
      "Configuration saved in ./sweeps/checkpoint-5000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to ./sweeps/checkpoint-5500\n",
      "Configuration saved in ./sweeps/checkpoint-5500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-5500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-6000\n",
      "Configuration saved in ./sweeps/checkpoint-6000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to ./sweeps/checkpoint-6500\n",
      "Configuration saved in ./sweeps/checkpoint-6500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-6500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-7000\n",
      "Configuration saved in ./sweeps/checkpoint-7000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-7000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./sweeps/checkpoint-7500\n",
      "Configuration saved in ./sweeps/checkpoint-7500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-7500/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-8000\n",
      "Configuration saved in ./sweeps/checkpoint-8000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-8000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▅▇▇▇██</td></tr><tr><td>eval/loss</td><td>█▆▄▁▂▄▂▂</td></tr><tr><td>eval/runtime</td><td>█▁▂▁▁▄▅█</td></tr><tr><td>eval/samples_per_second</td><td>▁█▇██▅▄▁</td></tr><tr><td>eval/steps_per_second</td><td>▁█▆██▆▄▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▆▅▅▃▃▂▅▆▁▃▃▄▆▃▄▅▁▇▆▂▂▂▂▂▆▄▆▂▃▃▁▁▄▃▄▃▃█▁▆</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.8635</td></tr><tr><td>eval/loss</td><td>0.34258</td></tr><tr><td>eval/runtime</td><td>138.9414</td></tr><tr><td>eval/samples_per_second</td><td>28.789</td></tr><tr><td>eval/steps_per_second</td><td>3.599</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>8000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1482</td></tr><tr><td>train/total_flos</td><td>1.683700265459712e+16</td></tr><tr><td>train/train_loss</td><td>0.35136</td></tr><tr><td>train/train_runtime</td><td>4102.7074</td></tr><tr><td>train/train_samples_per_second</td><td>15.598</td></tr><tr><td>train/train_steps_per_second</td><td>1.95</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-sweep-1</strong> at: <a href='https://wandb.ai/4-rldur0/nsmc_klue/runs/avx5fdkm' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/runs/avx5fdkm</a><br/> View project at: <a href='https://wandb.ai/4-rldur0/nsmc_klue' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240719_055419-avx5fdkm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bncbljgi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.617277189439932e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/AIFFEL-Quest/huggingface/wandb/run-20240719_070256-bncbljgi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/4-rldur0/nsmc_klue/runs/bncbljgi' target=\"_blank\">logical-sweep-2</a></strong> to <a href='https://wandb.ai/4-rldur0/nsmc_klue' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/4-rldur0/nsmc_klue' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/4-rldur0/nsmc_klue/runs/bncbljgi' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/runs/bncbljgi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 31996\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 8000\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1098' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1098/8000 09:29 < 59:44, 1.93 it/s, Epoch 0.27/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.577700</td>\n",
       "      <td>0.501703</td>\n",
       "      <td>0.781750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to ./sweeps/checkpoint-500\n",
      "Configuration saved in ./sweeps/checkpoint-500/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./sweeps/checkpoint-1000\n",
      "Configuration saved in ./sweeps/checkpoint-1000/config.json\n",
      "Model weights saved in ./sweeps/checkpoint-1000/pytorch_model.bin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project='nsmc_klue')\n",
    "\n",
    "wandb.agent(sweep_id, train, count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e015ad6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/AIFFEL-Quest/huggingface/wandb/run-20240719_071646-bncbljgi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/4-rldur0/nsmc_klue/runs/bncbljgi' target=\"_blank\">logical-sweep-2</a></strong> to <a href='https://wandb.ai/4-rldur0/nsmc_klue' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/4-rldur0/nsmc_klue' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/sweeps/8cvin4xn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/4-rldur0/nsmc_klue/runs/bncbljgi' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/runs/bncbljgi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "file /aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0/config.json not found\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py\", line 546, in get_config_dict\n",
      "    resolved_config_file = cached_path(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/file_utils.py\", line 1417, in cached_path\n",
      "    raise EnvironmentError(f\"file {url_or_filename} not found\")\n",
      "OSError: file /aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0/config.json not found\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_671/250046739.py\", line 12, in <module>\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 396, in from_pretrained\n",
      "    config, kwargs = AutoConfig.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\", line 527, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py\", line 570, in get_config_dict\n",
      "    raise EnvironmentError(msg)\n",
      "OSError: Can't load config for '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0'. Make sure that:\n",
      "\n",
      "- '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0' is a correct model identifier listed on 'https://huggingface.co/models'\n",
      "\n",
      "- or '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0' is the correct path to a directory containing a config.json file\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">logical-sweep-2</strong> at: <a href='https://wandb.ai/4-rldur0/nsmc_klue/runs/bncbljgi' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue/runs/bncbljgi</a><br/> View project at: <a href='https://wandb.ai/4-rldur0/nsmc_klue' target=\"_blank\">https://wandb.ai/4-rldur0/nsmc_klue</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240719_071646-bncbljgi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0'. Make sure that:\n\n- '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             resolved_config_file = cached_path(\n\u001b[0m\u001b[1;32m    547\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;31m# File, but it doesn't exist.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1417\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"file {url_or_filename} not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1418\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: file /aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0/config.json not found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_671/250046739.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#  using the same model class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     model = AutoModelForSequenceClassification.from_pretrained(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    397\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \"\"\"\n\u001b[1;32m    526\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"- or '{revision}' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models'\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeDecodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load config for '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0'. Make sure that:\n\n- '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or '/aiffel/aiffel/AIFFEL-Quest/huggingface/artifacts/run-c8ifvt62-history:v0' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "with wandb.init(project=\"nsmc_klue\") as run:\n",
    "    # Pass the name and version of Artifact\n",
    "    my_model_name = \"run-c8ifvt62-history:latest\"\n",
    "    my_model_artifact = run.use_artifact(my_model_name)\n",
    "\n",
    "    # Download model weights to a folder and return the path\n",
    "    model_dir = my_model_artifact.download()\n",
    "\n",
    "    # Load your Hugging Face model from that folder\n",
    "    #  using the same model class\n",
    "    num_labels = 2  \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dir, num_labels=num_labels\n",
    "    )\n",
    "    # Define evaluation arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        per_device_eval_batch_size=16,\n",
    "        do_train=False,\n",
    "        do_eval=True,\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset=tokenized_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = trainer.evaluate()\n",
    "    \n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
